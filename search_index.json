[["index.html", "Data Sciences Across the Disciplines: Geography Chapter 1 Introduction", " Data Sciences Across the Disciplines: Geography Joseph Holler Alex Lyford 2025-01-29 Chapter 1 Introduction This book is a guide to spatial data science built in collaboration with Professor Alex Lyford’s Data Science Across the Disciplines course at Middlebury College. Many thanks to Alex for designing the DSAD course! "],["r-and-calculations.html", "Chapter 2 R and Calculations 2.1 R is a calculator 2.2 Functions 2.3 Store objects for future use in environment 2.4 Mean function for averages", " Chapter 2 R and Calculations invent good folder structures for this class do not save all of this on desktop / downloads 1 + 3 ## [1] 4 2 * 8 ## [1] 16 2.1 R is a calculator click in a row of code to run it, or highlight chunks of code to run it ctrl + enter to run a block R does not care about white space 1+ 4 ## [1] 5 We suggest using a readable style for your code I suggest STYLR 2.2 Functions for most things in R, we will use functions functions need inputs (aka arguments), and output something outputs can vary drastically, and may include graphs, datasets, animations, etc. every function has a name, e.g. c for example, the c function stands for concatenate a vector (a list of things) use parenthesis after the function name to list arguments/inputs c(1,2,3) ## [1] 1 2 3 c(1, 2, 3) ## [1] 1 2 3 Code looks best with oxford commas (space after each item) 2.3 Store objects for future use in environment the vectors above were shown in the console pane, but not saved anywhere make a vector and and store it as vector1 object vector1 = c(1, 2, 3) notice that vector1 was added to the environment pane Access the object you stored vector1 ## [1] 1 2 3 the preferred way to assign a new object is &lt;- This avoids the mathematical meaning of = vector1 &lt;- c(1, 2, 3) 2.4 Mean function for averages google or ChatGPT: “how do I take an average in R?” Internet is great for finding function names / packages Use ? to find help on something, e.g. ?mean Help opens in the help pane ?mean In a function, the equals sign temporarily assigns inputs to arguments for the purposes of the function mean(x = vector1 ) ## [1] 2 you do not always need to label arguments labeling arguments can make code more readable without labels, R assumes arguments are provided in predetermined order (as shown in help) Some parameters may have default values as shown in help, e.g. mean(x, trim = 0, na.rm = FALSE, ...) story of defaults in a logistic regression function that caused problems in research trim would involve removing a proportion of values; na.rm would involve NA missing values mean(vector1) ## [1] 2 make a vector with missing values NA is shown in color, having special meaning missing_vector &lt;- c(5, 9, 14, NA) missing_vector ## [1] 5 9 14 NA suggests creating descriptive object names mean(x = missing_vector) ## [1] NA the answer is not available! We cannot include missing values in math try using the option to remove missing values TRUE is a special word for the logical value “true” mean(x= missing_vector, na.rm = TRUE) ## [1] 9.333333 aha, now the mean is calculated based on the three known values because we added a parameter to tell the mean function to ignore missing data "],["graph-values-and-qualities-of-diamonds.html", "Chapter 3 Graph values and qualities of diamonds 3.1 Install the tidyverse package 3.2 Explore Diamonds Dataset 3.3 Facet the data 3.4 Restart R and Remake last graph 3.5 Explore gap in prices between 1k and 2k", " Chapter 3 Graph values and qualities of diamonds there are many different functions / ways to explore data and graph it these may be the most common/efficient/best ways, but there might be more specialized ways out there 3.1 Install the tidyverse package install the tidyverse package with install.packages install.packages(&quot;tidyverse&quot;) use library function to load installed packages into R needs to be run every time you start an R session more efficient for R to load only the packages you need for a particular session / analysis 3.2 Explore Diamonds Dataset diamonds is a dataset installed along with tidyverse diamonds ## # A tibble: 53,940 × 10 ## carat cut color clarity depth table price x y z ## &lt;dbl&gt; &lt;ord&gt; &lt;ord&gt; &lt;ord&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0.23 Ideal E SI2 61.5 55 326 3.95 3.98 2.43 ## 2 0.21 Premium E SI1 59.8 61 326 3.89 3.84 2.31 ## 3 0.23 Good E VS1 56.9 65 327 4.05 4.07 2.31 ## 4 0.29 Premium I VS2 62.4 58 334 4.2 4.23 2.63 ## 5 0.31 Good J SI2 63.3 58 335 4.34 4.35 2.75 ## 6 0.24 Very Good J VVS2 62.8 57 336 3.94 3.96 2.48 ## 7 0.24 Very Good I VVS1 62.3 57 336 3.95 3.98 2.47 ## 8 0.26 Very Good H SI1 61.9 55 337 4.07 4.11 2.53 ## 9 0.22 Fair E VS2 65.1 61 337 3.87 3.78 2.49 ## 10 0.23 Very Good H VS1 59.4 61 338 4 4.05 2.39 ## # ℹ 53,930 more rows Let’s graph the weight of a diamond against its $ value using the ggplot function Call up help on a function or package with a ? followed by a function name in the console. For example, ?ggplot brings up help on the graph plotting function. creates a graphing object, in which all arguments are optional ggplot technically does not need any parameters (to make a blank graph graphing is like painting layers on a canvas ggplot() starts a blank graphing canvas, on which you add data and aesthetic options. add layers to ggplot with + check out ggplot cheat sheet pdf from posit.co help documentation and cheat sheet shows what you can customize let’s make a scatterplot with geom_point we need to give a dataset to ggplot and define x and y coordinates aes tells R to make aesthetics using columns in your dataset ggplot(data = diamonds) + geom_point(aes(x = carat, y = price)) Let’s interpret the graph price increases with carat weight clustering of data on thresholds: 1 carat, 1.5 carats, 2 carats you need domain knowledge (content expertise) to explain patterns more variation in price as weight increases Reflection Every graph will have this fundamental code structure, with modifications copy code you have that works and modify what you need! ggplot(data = diamonds) + geom_point(aes(x = depth, y = price)) Try graphing three variables: diamond color, carat and price. How could we approach this? use scatterplot and symbolize the dots ggplot(data = arrange(diamonds, desc(color))) + geom_point(aes(x = carat, y = price, color = color)) Interpret: D appears more valuable than J Few instances of very large D diamonds Note that ggplot adds each color series one by one many +’s in console indicates one of your lines of code is incomplete (missing parenthesis, started line with a +, etc.) 3.3 Facet the data subdivide the data by a variable &amp; create different graphs for each subgroup ggplot(data = diamonds) + geom_point(aes(x = carat, y = price, color = color)) + facet_wrap(~color) Interpret: now it’s possible to analyze trends of each color separately maybe facet by cut instead ggplot(data = diamonds) + geom_point(aes(x = carat, y = price, color = color)) + facet_wrap(~cut) Interpret: as cut improves, price gets better price improves faster by carat for better cuts ggplot(data = diamonds) + geom_point(aes(x = carat, y = price, color = color)) + facet_grid(clarity~cut) Interpret: Each column represents a cut, and each row represents a color This visualization shows 5 variables IF is more clear (valuable) than I1 3.4 Restart R and Remake last graph If you get an error about a function not being found, you either mistyped its name or more likely, you need to load the library in which the function is found. You need to do this every time you open a new R session. ggplot(data = diamonds) + geom_point(aes(x = carat, y = price, color = color)) + facet_grid(clarity~cut) ctrl+shift+r can add a section to an R script. Headings take the place of that in Rmd 3.5 Explore gap in prices between 1k and 2k Let’s make a histogram showing the distribution of a single quantitative variable ggplot(data = diamonds) + geom_histogram(aes(x = price)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. There’s your basic histogram. Graph shows the aggregation of diamond data; each bar’s width represents a price range. and its height represents the number of data points within that price range. Interpretation: the vast majority of diamonds cost less than 2k, and a few diamonds cost more We can optionally change the fill and outline colors. ggplot(data = diamonds) + geom_histogram(aes(x = price), fill = &quot;darkblue&quot;, color = &quot;black&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Let’s “zoom in” on the area of data with a gap in prices by changing the limits of the x-axis to 1000 and 2000 ggplot(data = diamonds) + geom_histogram(aes(x = price), fill = &quot;darkblue&quot;, color = &quot;black&quot;) + xlim(1000, 2000) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 44232 rows containing non-finite outside the scale range ## (`stat_bin()`). ## Warning: Removed 2 rows containing missing values or values outside the scale ## range (`geom_bar()`). Data science cannot tell us why this price gap exists: you need domain knowledge to explain. "],["wrangle-and-graph-new-york-city-flights.html", "Chapter 4 Wrangle and graph New York City flights 4.1 Which carriers should travelers avoid? 4.2 Data Wrangling 4.3 On average, how long are flights delayed? 4.4 Pipe Operator 4.5 Warmup Question", " Chapter 4 Wrangle and graph New York City flights Install the nycflights23 package, containing data on flights in and out of New York City’s airports in 2023. The package does not contain any functions, just data. install.packages(&quot;nycflights23&quot;) Load the nycflights23 package view flights flights ## # A tibble: 435,352 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2023 1 1 1 2038 203 328 3 ## 2 2023 1 1 18 2300 78 228 135 ## 3 2023 1 1 31 2344 47 500 426 ## 4 2023 1 1 33 2140 173 238 2352 ## 5 2023 1 1 36 2048 228 223 2252 ## 6 2023 1 1 503 500 3 808 815 ## 7 2023 1 1 520 510 10 948 949 ## 8 2023 1 1 524 530 -6 645 710 ## 9 2023 1 1 537 520 17 926 818 ## 10 2023 1 1 547 545 2 845 852 ## # ℹ 435,342 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; 4.1 Which carriers should travelers avoid? Analysis goal: which flight carriers should travelers avoid? Let’s explore what goes on at different airports with different carriers. NYTIMES story about change in delay time! How often is a flight delayed? This answer may be subjective. Let’s define flight delays as arriving any amount of time after scheduled Make a histogram of arrival delays. pressing “tab” can open an auto-fill menu ggplot(flights) + geom_histogram(aes(x = arr_delay), fill = &quot;darkblue&quot;, color = &quot;grey&quot;) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 12534 rows containing non-finite outside the scale range ## (`stat_bin()`). Let’s limit the graph to the section with the most data ggplot(flights) + geom_histogram(aes(x = arr_delay), fill = &quot;darkblue&quot;, color = &quot;grey&quot;) + xlim(-100, 300) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 14636 rows containing non-finite outside the scale range ## (`stat_bin()`). ## Warning: Removed 2 rows containing missing values or values outside the scale ## range (`geom_bar()`). 4.2 Data Wrangling Manipulating data… Let’s calculate the proportion of delayed flights Data wrangling functions are English words that suggest what you’re doing with the data Create a new column indicating whether flight was delayed mutate function does this to a dataset. Below, we calculate a new column, was_flight_delayed and fill it in with TRUE if arr_delay is greater than 0, and FALSE if it is not. flights_with_delay &lt;- mutate(flights, was_flight_delayed = arr_delay &gt; 0) flights_with_delay ## # A tibble: 435,352 × 20 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2023 1 1 1 2038 203 328 3 ## 2 2023 1 1 18 2300 78 228 135 ## 3 2023 1 1 31 2344 47 500 426 ## 4 2023 1 1 33 2140 173 238 2352 ## 5 2023 1 1 36 2048 228 223 2252 ## 6 2023 1 1 503 500 3 808 815 ## 7 2023 1 1 520 510 10 948 949 ## 8 2023 1 1 524 530 -6 645 710 ## 9 2023 1 1 537 520 17 926 818 ## 10 2023 1 1 547 545 2 845 852 ## # ℹ 435,342 more rows ## # ℹ 12 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, was_flight_delayed &lt;lgl&gt; count function counts things! count(flights_with_delay, was_flight_delayed) ## # A tibble: 3 × 2 ## was_flight_delayed n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 281244 ## 2 TRUE 141574 ## 3 NA 12534 The missing values are cancelled flights! It’s important to investigate NAs: What do they mean? Why are they missing? What should we do with this information in the context of the research question? 4.3 On average, how long are flights delayed? summarize allows us to calculate summary statistics summarize(flights, average_flight_delay = mean(arr_delay, na.rm=TRUE)) ## # A tibble: 1 × 1 ## average_flight_delay ## &lt;dbl&gt; ## 1 4.34 Out of all flights, the average delay is 4.34 But of the flights that are delayed, what is the average delay time? filter can filter out rows of the dataset only_delayed_flights &lt;- filter(flights, arr_delay &gt; 0) only_delayed_flights ## # A tibble: 141,574 × 19 ## year month day dep_time sched_dep_time dep_delay arr_time sched_arr_time ## &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 2023 1 1 1 2038 203 328 3 ## 2 2023 1 1 18 2300 78 228 135 ## 3 2023 1 1 31 2344 47 500 426 ## 4 2023 1 1 33 2140 173 238 2352 ## 5 2023 1 1 36 2048 228 223 2252 ## 6 2023 1 1 537 520 17 926 818 ## 7 2023 1 1 549 559 -10 905 901 ## 8 2023 1 1 605 600 5 906 855 ## 9 2023 1 1 611 530 41 923 839 ## 10 2023 1 1 619 620 -1 751 745 ## # ℹ 141,564 more rows ## # ℹ 11 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;, ## # tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;, ## # hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt; Here are just the 141k… flights that were delayed Shall we now summarize the average delay of the delayed flights ? summarize(only_delayed_flights, average_flight_delay = mean(arr_delay, na.rm=TRUE)) ## # A tibble: 1 × 1 ## average_flight_delay ## &lt;dbl&gt; ## 1 50.3 Aha, there’s an average of 50 minutes delay! What is the average flight delay time for each carrier? group_by allows you to calculate summaries for subgroups of the data all at once First, let’s group the flight record by carrier flights_grouped_by_carrier &lt;- group_by(flights, carrier) Next, let’s summarize average delay time summarize(flights_grouped_by_carrier, average_flight_delay = mean(arr_delay, na.rm=TRUE)) ## # A tibble: 14 × 2 ## carrier average_flight_delay ## &lt;chr&gt; &lt;dbl&gt; ## 1 9E -2.23 ## 2 AA 5.27 ## 3 AS 0.0844 ## 4 B6 15.6 ## 5 DL 1.64 ## 6 F9 26.2 ## 7 G4 -5.88 ## 8 HA 21.4 ## 9 MQ 0.119 ## 10 NK 9.89 ## 11 OO 13.7 ## 12 UA 9.04 ## 13 WN 5.76 ## 14 YX -4.64 When data wrangling, we can chain multiple functions together in a sequence 4.4 Pipe Operator It looks like this: |&gt; ctrl + shift + m (change global options -&gt; code to use this) The pipe takes the result of whatever is on the left, and inserts it as the first argument to the next function. More efficiently, we can group and summarize all at once. flights |&gt; group_by(carrier) |&gt; summarize(average_flight_delay = mean(arr_delay, na.rm=TRUE)) ## # A tibble: 14 × 2 ## carrier average_flight_delay ## &lt;chr&gt; &lt;dbl&gt; ## 1 9E -2.23 ## 2 AA 5.27 ## 3 AS 0.0844 ## 4 B6 15.6 ## 5 DL 1.64 ## 6 F9 26.2 ## 7 G4 -5.88 ## 8 HA 21.4 ## 9 MQ 0.119 ## 10 NK 9.89 ## 11 OO 13.7 ## 12 UA 9.04 ## 13 WN 5.76 ## 14 YX -4.64 We can modify that sequence to also filter by flights that are in fact delayed. Marcus asked about difference between spacing lines out with commas (for many parameters) and spacing out with piping flights |&gt; filter(arr_delay &gt; 0) |&gt; group_by(carrier) |&gt; summarize(average_flight_delay = mean(arr_delay, na.rm=TRUE)) ## # A tibble: 14 × 2 ## carrier average_flight_delay ## &lt;chr&gt; &lt;dbl&gt; ## 1 9E 43.9 ## 2 AA 55.9 ## 3 AS 47.7 ## 4 B6 58.1 ## 5 DL 52.0 ## 6 F9 67.5 ## 7 G4 43.2 ## 8 HA 45.0 ## 9 MQ 27.7 ## 10 NK 54.7 ## 11 OO 60.7 ## 12 UA 50.1 ## 13 WN 36.8 ## 14 YX 39.8 4.5 Warmup Question What is the average flight time in the air of Delta flights leaving each of the three NYC airports? Answer with a graph == is the logical comparison “equals to” whereas = assigns a new value \"DL\" is in quotes because it is a text string need to use mean() inside of summarize() because the input is a full dataset, not just a vector list need to use geom_col() rather than geom_bar() because we already know the y value. geom_bar() is for when you still need to count the values flights |&gt; filter(carrier == &quot;DL&quot;) |&gt; group_by(origin) |&gt; summarize(average_air_time = mean(air_time, na.rm = TRUE)) |&gt; ggplot() + geom_col(aes(x = origin, y = average_air_time)) Lets’s create this column graph for all the carriers Group by both origin and carrier flights |&gt; #filter(carrier == &quot;DL&quot;) |&gt; group_by(origin, carrier) |&gt; summarize(average_air_time = mean(air_time, na.rm = TRUE)) |&gt; ggplot() + geom_col(aes(x = origin, y = average_air_time)) + facet_wrap(~carrier) ## `summarise()` has grouped output by &#39;origin&#39;. You can override using ## the `.groups` argument. "],["okcupid.html", "Chapter 5 OkCupid 5.1 Load OK Cupid Data 5.2 Height 5.3 Age 5.4 Diet and Body Type 5.5 Save dataset to computer 5.6 Heat map of diet and body type 5.7 Text mining", " Chapter 5 OkCupid Install the here, tidyverse and scales packages. install.packages(&quot;here&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;scales&quot;) Load tidyverse package, plus the three packages above. 5.1 Load OK Cupid Data Set up a path to store the data In Rmarkdown, a line with just a data object will print out that data object. The data_private folder is included in our .gitignore file such that Git neither tracks nor attempts to push this large and sensitive data file to GitHub.com. if(!dir.exists(here(&quot;data_private&quot;))) {dir.create(here(&quot;data_private&quot;))} # set file path okCupidFilePath &lt;- here(&quot;data_private&quot;, &quot;profiles.csv&quot;) # show the file path okCupidFilePath ## [1] &quot;C:/GitHub/opengisci/wt25_josephholler/data_private/profiles.csv&quot; Download the file if it is not stored locally already. Note that this code will not work indefinitely: the file will be removed once the course concludes. if(!file.exists(okCupidFilePath)){ download.file(&quot;https://geography.middlebury.edu/jholler/wt25/profiles.csv&quot;, okCupidFilePath) } You can use read_csv to load CSV files into R. The file.choose() function allows you to interactively pick a file, but this is not reproducible. okcupid_data &lt;- read_csv(file.choose()) Even better, use the here package for reproducible file paths okcupid_data &lt;- read_csv(okCupidFilePath) Save the OK Cupid data in R’s compressed RDS data format cupidRDSpath &lt;- here(&quot;data_private&quot;, &quot;okCupid.RDS&quot;) saveRDS(okcupid_data, cupidRDSpath) Load the OK Cupid data from the RDS file okcupid_data &lt;- readRDS(cupidRDSpath) 5.2 Height How do people report their height? okcupid_data |&gt; ggplot() + geom_histogram(aes(x = height)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ## Warning: Removed 3 rows containing non-finite outside the scale range ## (`stat_bin()`). Clearly there are some inaccurate self-reported data in here, so lets’ restrict the range of the x-axis Tough decisions to make when graphing, so you need to find a balance in the data visualization goals and exclusion of some individuals from the data. Switch to geom_bar() to make one bar for each integer value in a bar chart okcupid_data |&gt; ggplot() + geom_bar(aes(x = height)) + xlim(55, 80) There are two different peaks for each gender’s mean, so let’s facet the graphs okcupid_data |&gt; ggplot() + geom_bar(aes(x = height)) + xlim(55, 80) + facet_wrap(~sex) There are more men than women on OKCupid. There are spikes rounding up to appealing heights. 5.3 Age okcupid_data |&gt; ggplot() + geom_bar(aes(x = age)) + facet_wrap(~sex) There are more men, but the distributions are very similar. How can we compare the two datasets to see if their patterns are similar? Try a density plot coded by color fill. Add transparency so that you can see. I would prefer to change the color (line) rather than the fill okcupid_data |&gt; ggplot() + geom_density(aes(x = age, fill = sex), alpha=0.5) 5.4 Diet and Body Type Count diet responses okcupid_data |&gt; count(diet) ## # A tibble: 19 × 2 ## diet n ## &lt;chr&gt; &lt;int&gt; ## 1 anything 6183 ## 2 halal 11 ## 3 kosher 11 ## 4 mostly anything 16585 ## 5 mostly halal 48 ## 6 mostly kosher 86 ## 7 mostly other 1007 ## 8 mostly vegan 338 ## 9 mostly vegetarian 3444 ## 10 other 331 ## 11 strictly anything 5113 ## 12 strictly halal 18 ## 13 strictly kosher 18 ## 14 strictly other 452 ## 15 strictly vegan 228 ## 16 strictly vegetarian 875 ## 17 vegan 136 ## 18 vegetarian 667 ## 19 &lt;NA&gt; 24395 Count body type responses okcupid_data |&gt; count(body_type) ## # A tibble: 13 × 2 ## body_type n ## &lt;chr&gt; &lt;int&gt; ## 1 a little extra 2629 ## 2 athletic 11819 ## 3 average 14652 ## 4 curvy 3924 ## 5 fit 12711 ## 6 full figured 1009 ## 7 jacked 421 ## 8 overweight 444 ## 9 rather not say 198 ## 10 skinny 1777 ## 11 thin 4711 ## 12 used up 355 ## 13 &lt;NA&gt; 5296 We probably want to condense these categories to simplify graphing 5.4.1 Collapse Diet Categories Use case_when to switch (simplify) diet type values okcupid_data |&gt; mutate(diet_collapsed = case_when(str_detect(diet, &quot;halal&quot;) ~ &quot;halal&quot;, str_detect(diet, &quot;kosher&quot;) ~ &quot;kosher&quot;, str_detect(diet, &quot;other&quot;) ~ &quot;other&quot;, str_detect(diet, &quot;vegan&quot;) ~ &quot;vegan&quot;, str_detect(diet, &quot;anything&quot;) ~ &quot;anything&quot;, str_detect(diet, &quot;vegetarian&quot;) ~ &quot;vegetarian&quot;)) |&gt; count(diet_collapsed, body_type) |&gt; ggplot() + geom_col(aes(y = body_type, x = n, fill = diet_collapsed), position = &quot;fill&quot;) 5.5 Save dataset to computer First save a data object in the environment okcupid_data_with_diet_collapsed &lt;- okcupid_data |&gt; mutate(diet_collapsed = case_when(str_detect(diet, &quot;halal&quot;) ~ &quot;halal&quot;, str_detect(diet, &quot;kosher&quot;) ~ &quot;kosher&quot;, str_detect(diet, &quot;other&quot;) ~ &quot;other&quot;, str_detect(diet, &quot;vegan&quot;) ~ &quot;vegan&quot;, str_detect(diet, &quot;anything&quot;) ~ &quot;anything&quot;, str_detect(diet, &quot;vegetarian&quot;) ~ &quot;vegetarian&quot;)) Then save to a CSV file. By default, this would save to the working directory. The working directory can be changed with the session : set working directory option. However, an even better solution is to use the here package. write_csv(okcupid_data_with_diet_collapsed, here(&quot;data_private&quot;, &quot;okcupid_with_diet_collapsed.csv&quot;)) 5.6 Heat map of diet and body type We can change the scale of fill okcupid_data_with_diet_collapsed |&gt; count(diet_collapsed, body_type) |&gt; ggplot() + geom_tile(aes(x = diet_collapsed, y = body_type, fill = n)) This data would look better on a log scale, which can be achieved with the log10_trans() function from the scales package. okcupid_data_with_diet_collapsed |&gt; count(diet_collapsed, body_type) |&gt; ggplot() + geom_tile(aes(x = diet_collapsed, y = body_type, fill = n)) + scale_fill_gradient(trans = log10_trans()) 5.7 Text mining anonymize the data consider ethics / legality of reposting and how to repost 5.7.1 Essay data dictionary Variable Description essay0 My self summary essay1 What I’m doing with my life essay2 I’m really good at essay3 The first thing people usually notice about me essay4 Favorite books, movies, show, music, and food essay5 The six things I could never do without essay6 I spend a lot of time thinking about essay7 On a typical Friday night I am essay8 The most private thing I am willing to admit essay9 You should message me if… 5.7.2 What are people really good at? This is stored in essay2 Let’s look at a few responses With markdown, you can keep re-running the cell okcupid_data |&gt; sample_n(1) |&gt; pull(essay2) ## [1] &quot;i was a national certified massage therapist so i can be really\\r\\ngood at massage therapy. plus im good at writing songs.&quot; Let’s scale up the text analysis How many people are good at sports? str_detect searches for a text string including | means or okcupid_data |&gt; mutate(is_good_at_sports = str_detect(essay2, &quot;sports|soccer|baseball|football|basketball&quot;)) |&gt; count(is_good_at_sports) ## # A tibble: 3 × 2 ## is_good_at_sports n ## &lt;lgl&gt; &lt;int&gt; ## 1 FALSE 47737 ## 2 TRUE 2571 ## 3 NA 9638 What is the relationship between body type and people who say they are good at sports? okcupid_data |&gt; mutate(is_good_at_sports = str_detect(essay2, &quot;sports|soccer|baseball|football|basketball&quot;)) |&gt; count(is_good_at_sports, body_type) |&gt; ggplot() + geom_col(aes(y = body_type, x = n, fill = is_good_at_sports), position = &quot;fill&quot;) Remove the missing data with filter(!is.na(essay2)) okcupid_data |&gt; mutate(is_good_at_sports = str_detect(essay2, &quot;sports|soccer|baseball|football|basketball&quot;)) |&gt; filter(!is.na(essay2)) |&gt; count(is_good_at_sports, body_type) |&gt; ggplot() + geom_col(aes(y = body_type, x = n, fill = is_good_at_sports), position = &quot;fill&quot;) People who say they’re athletic or jacked are more likely to say they’re good at some sport okcupid_data |&gt; mutate(is_good_at_sports = str_detect(essay2, &quot;sports|soccer|baseball|football|basketball&quot;)) |&gt; filter(!is.na(essay2)) |&gt; count(is_good_at_sports, body_type) |&gt; ggplot() + geom_col(aes(y = is_good_at_sports, x = n, fill = body_type), position = &quot;fill&quot;) A large proportion of the folks who mentioned being good at a sport thought they had an athletic body type 5.7.3 Analyze word usage in essays okcupid_data |&gt; unnest_tokens(input = essay2, output = &quot;word&quot;) |&gt; count(word) ## # A tibble: 33,824 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 0 41 ## 2 0.13 1 ## 3 0.334 1 ## 4 0.99 1 ## 5 00 6 ## 6 000000 1 ## 7 007 2 ## 8 00am 1 ## 9 00s 2 ## 10 00t 1 ## # ℹ 33,814 more rows Let’s arrange the dataset to sort by n Also filter by stop_words essay2word_counts &lt;- okcupid_data |&gt; unnest_tokens(input = essay2, output = &quot;word&quot;, format = &quot;html&quot;) |&gt; count(word) |&gt; arrange(-n) |&gt; filter(!(word %in% stop_words$word)) essay2word_counts ## # A tibble: 31,253 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 people 15118 ## 2 cooking 7042 ## 3 listening 6497 ## 4 laugh 4776 ## 5 love 4400 ## 6 friends 3961 ## 7 time 3899 ## 8 pretty 3368 ## 9 finding 3322 ## 10 playing 3157 ## # ℹ 31,243 more rows Interestingly, I’m getting results of conjunctions because I used the html filter, but those could be filtered out as well. essay2word_counts |&gt; filter(!(word %in% c(&quot;br&quot;, &quot;href&quot;, &quot;ilink&quot;, &quot;ve&quot;, &quot;don&quot;)), !is.na(word)) |&gt; head(20) |&gt; ggplot() + geom_col(aes(y= reorder(word, n), x = n)) Try again with essay 8 essay8word_counts &lt;- okcupid_data |&gt; unnest_tokens(input = essay8, output = &quot;word&quot;) |&gt; count(word) |&gt; arrange(-n) |&gt; filter(!(word %in% stop_words$word)) essay8word_counts ## # A tibble: 29,029 × 2 ## word n ## &lt;chr&gt; &lt;int&gt; ## 1 &lt;NA&gt; 19214 ## 2 br 16439 ## 3 private 3858 ## 4 love 2958 ## 5 people 2420 ## 6 time 2095 ## 7 admit 1687 ## 8 person 1656 ## 9 life 1488 ## 10 pretty 1480 ## # ℹ 29,019 more rows This time, do it the same way as shown, without altering the mode. essay8word_counts |&gt; filter(!(word %in% c(&quot;br&quot;, &quot;href&quot;, &quot;ilink&quot;)), !is.na(word)) |&gt; head(20) |&gt; ggplot() + geom_col(aes(y= reorder(word, n), x = n)) This Rmd did not initially knit without separate installation of hunspell package. "],["baby-names.html", "Chapter 6 Baby Names 6.1 Bens over time 6.2 Babies over time", " Chapter 6 Baby Names Install here package to help manage file paths install.packages(&quot;here&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;babynames&quot;) Load tidyverse and here packages library(tidyverse) library(here) library(babynames) ## Warning: package &#39;babynames&#39; was built under R version 4.3.3 6.1 Bens over time babynames |&gt; filter(name %in% c(&quot;Ben&quot;, &quot;Benjamin&quot;, &quot;Benji&quot;)) |&gt; group_by(year, sex) |&gt; summarize(total_ben = sum(n)) |&gt; ggplot() + geom_line(aes(x = year, y = total_ben)) + facet_wrap(~sex) ## `summarise()` has grouped output by &#39;year&#39;. You can override using ## the `.groups` argument. 6.2 Babies over time How many babies are there overall? babynames |&gt; #filter(name %in% c(&quot;Ben&quot;, &quot;Benjamin&quot;, &quot;Benji&quot;)) |&gt; group_by(year, sex) |&gt; summarize(total_ben = sum(n)) |&gt; ggplot() + geom_line(aes(x = year, y = total_ben)) + facet_wrap(~sex) ## `summarise()` has grouped output by &#39;year&#39;. You can override using ## the `.groups` argument. "],["web-scraping.html", "Chapter 7 Web Scraping 7.1 Scrape all paragraph text from a webpage 7.2 Scrape a specific item from a webpage 7.3 Ethics of web scraping 7.4 Scrape data 7.5 Cardinal sin of data science", " Chapter 7 Web Scraping Install and load packages Install here package to help manage file paths install.packages(&quot;here&quot;) install.packages(&quot;tidyverse&quot;) install.packages(&quot;rvest&quot;) Load tidyverse and here packages library(tidyverse) library(here) library(rvest) ## Warning: package &#39;rvest&#39; was built under R version 4.3.3 ## ## Attaching package: &#39;rvest&#39; ## The following object is masked from &#39;package:readr&#39;: ## ## guess_encoding urloi &lt;- &quot;https://en.wikipedia.org/wiki/Dolphin&quot; 7.1 Scrape all paragraph text from a webpage html_text cleans up html tags out of text html_elements selects content within particular html tags. p is for paragraph. all_text &lt;- urloi |&gt; read_html() |&gt; html_elements(&quot;p&quot;) |&gt; html_text() all_text %&gt;% head(5) ## [1] &quot;\\n\\n&quot; ## [2] &quot;A dolphin is an aquatic mammal in the clade Odontoceti (toothed whale). Dolphins belong to the families Delphinidae (the oceanic dolphins), Platanistidae (the Indian river dolphins), Iniidae (the New World river dolphins), Pontoporiidae (the brackish dolphins), and possibly extinct Lipotidae (baiji or Chinese river dolphin). There are 40 extant species named as dolphins.\\n&quot; ## [3] &quot;Dolphins range in size from the 1.7-metre-long (5 ft 7 in) and 50-kilogram (110-pound) Maui&#39;s dolphin to the 9.5 m (31 ft) and 10-tonne (11-short-ton) orca. Various species of dolphins exhibit sexual dimorphism where the males are larger than females. They have streamlined bodies and two limbs that are modified into flippers. Though not quite as flexible as seals, they are faster; some dolphins can briefly travel at speeds of 29 kilometres per hour (18 mph) or leap about 9 metres (30 ft).[1] Dolphins use their conical teeth to capture fast-moving prey. They have well-developed hearing which is adapted for both air and water; it is so well developed that some can survive even if they are blind. Some species are well adapted for diving to great depths. They have a layer of fat, or blubber, under the skin to keep warm in the cold water.\\n&quot; ## [4] &quot;Dolphins are widespread. Most species prefer the warm waters of the tropic zones, but some, such as the right whale dolphin, prefer colder climates. Dolphins feed largely on fish and squid, but a few, such as the orca, feed on large mammals such as seals. Male dolphins typically mate with multiple females every year, but females only mate every two to three years. Calves are typically born in the spring and summer months and females bear all the responsibility for raising them. Mothers of some species fast and nurse their young for a relatively long period of time. \\n&quot; ## [5] &quot;Dolphins produce a variety of vocalizations, usually in the form of clicks and whistles.\\n&quot; 7.2 Scrape a specific item from a webpage Right-click and inspect a page to investigate how it’s structured. Right-click an item and copy x-path If the path has quotes in it, use alternative form of quotes (single instead of double or vis a versa) /html/body/div/main/article/section/div/div/p[4] all_text &lt;- urloi |&gt; read_html() |&gt; html_element(xpath = &quot;/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/figure[1]/figcaption&quot;) |&gt; html_text() all_text ## [1] &quot; A common bottlenose dolphin (Tursiops truncatus)&quot; 7.3 Ethics of web scraping In the United States, web scraping is legal. However, there are many illegal things you may do with the data you scraped, including copyright infringement. Poor scraping design may also crash web servers through denial of service. add /robots.txt to a web address to see what you are not supposed to crawl. Web scrapers can add pauses into their code to detect scraping behavior. Rtweet was throttled and re-queried every 12 to 15 minutes. 7.4 Scrape data Scrape data on Tom Brady from Wikipedia. urloi &lt;- &quot;https://en.wikipedia.org/wiki/Tom_Brady&quot; Scrape table-based text from a page all_tables &lt;- urloi |&gt; read_html() |&gt; html_elements(&quot;table&quot;) # html_text() all_tables ## {xml_nodeset (58)} ## [1] &lt;table class=&quot;box-Very_long plainlinks metadata ambox ambox-style ambox- ... ## [2] &lt;table class=&quot;infobox vcard&quot; style=&quot;width: 25em; line-height: 1.2em;&quot;&gt;\\n ... ## [3] &lt;table class=&quot;infobox&quot; style=&quot;border-collapse:collapse; border-spacing:0 ... ## [4] &lt;table class=&quot;infobox&quot; style=&quot;border-collapse:collapse; border-spacing:0 ... ## [5] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center;&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th r ... ## [6] &lt;table class=&quot;box-Cleanup_reorganize plainlinks metadata ambox ambox-sty ... ## [7] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center&quot;&gt;\\n&lt;caption&gt;Pre-draft ... ## [8] &lt;table class=&quot;wikitable&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;&lt;th colspan=&quot;2&quot;&gt;Legend\\n&lt;/th&gt;&lt;/tr&gt; ... ## [9] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center;&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th r ... ## [10] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center;&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th r ... ## [11] &lt;table class=&quot;wikitable&quot; style=&quot;text-align:center;&quot;&gt;&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th r ... ## [12] &lt;table class=&quot;col-begin&quot; role=&quot;presentation&quot;&gt;&lt;tbody&gt;&lt;tr&gt;&lt;td class=&quot;col-b ... ## [13] &lt;table class=&quot;wikitable&quot;&gt;\\n&lt;caption&gt;\\n&lt;/caption&gt;\\n&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Yea ... ## [14] &lt;table class=&quot;wikitable&quot;&gt;\\n&lt;caption&gt;\\n&lt;/caption&gt;\\n&lt;tbody&gt;\\n&lt;tr&gt;\\n&lt;th&gt;Yea ... ## [15] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [16] &lt;table class=&quot;nowraplinks mw-collapsible mw-collapsed navbox-inner&quot; styl ... ## [17] &lt;table class=&quot;nowraplinks mw-collapsible mw-collapsed navbox-inner&quot; styl ... ## [18] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [19] &lt;table class=&quot;nowraplinks mw-collapsible autocollapse navbox-inner&quot; styl ... ## [20] &lt;table class=&quot;nowraplinks mw-collapsible mw-collapsed navbox-inner&quot; styl ... ## ... Hold on, scrape a specific table reg_season &lt;- urloi |&gt; read_html() |&gt; html_element(xpath = &quot;/html/body/div[2]/div/div[3]/main/div[3]/div[3]/div[1]/table[7]&quot;) |&gt; html_table() reg_season ## # A tibble: 25 × 23 ## Year Team Games Games Games Passing Passing Passing Passing Passing Passing ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Year Team GP GS Reco… Cmp Att Pct Yds Y/A Lng ## 2 2000 NE 1 0 — 1 3 33.3 6 2.0 6 ## 3 2001 NE 15 14 11–3 264 413 63.9 2,843 6.9 91 ## 4 2002 NE 16 16 9–7 373 601 62.1 3,764 6.3 49 ## 5 2003 NE 16 16 14–2 317 527 60.2 3,620 6.9 82 ## 6 2004 NE 16 16 14–2 288 474 60.8 3,692 7.8 50 ## 7 2005 NE 16 16 10–6 334 530 63.0 4,110 7.8 71 ## 8 2006 NE 16 16 12–4 319 516 61.8 3,529 6.8 62 ## 9 2007 NE 16 16 16–0 398 578 68.9 4,806 8.3 69 ## 10 2008 NE 1 1 1–0 7 11 63.6 76 6.9 26 ## # ℹ 15 more rows ## # ℹ 12 more variables: Passing &lt;chr&gt;, Passing &lt;chr&gt;, Passing &lt;chr&gt;, ## # Rushing &lt;chr&gt;, Rushing &lt;chr&gt;, Rushing &lt;chr&gt;, Rushing &lt;chr&gt;, Rushing &lt;chr&gt;, ## # Sacked &lt;chr&gt;, Sacked &lt;chr&gt;, Fumbles &lt;chr&gt;, Fumbles &lt;chr&gt; Summarize average First you need unique column names! You can make a list of new column names as follows: colnames(reg_season) &lt;- c(\"Year\", \"Team\", ... \"LastVar\") Better solution: paste data in row 1 into the column names Subset the first row using brackets [row(s), column(s)] 7.5 Cardinal sin of data science Overwrite a dataset that you cannot get back. The solution to this is save a raw version of your data and work with a new version of the data. Please save an RDS file! Paste with a sep option! brady_data_clean &lt;- reg_season paste(colnames(brady_data_clean), reg_season[1, ]) ## [1] &quot;Year Year&quot; &quot;Team Team&quot; &quot;Games GP&quot; &quot;Games GS&quot; &quot;Games Record&quot; ## [6] &quot;Passing Cmp&quot; &quot;Passing Att&quot; &quot;Passing Pct&quot; &quot;Passing Yds&quot; &quot;Passing Y/A&quot; ## [11] &quot;Passing Lng&quot; &quot;Passing TD&quot; &quot;Passing Int&quot; &quot;Passing Rtg&quot; &quot;Rushing Att&quot; ## [16] &quot;Rushing Yds&quot; &quot;Rushing Y/A&quot; &quot;Rushing Lng&quot; &quot;Rushing TD&quot; &quot;Sacked Sck&quot; ## [21] &quot;Sacked Yds&quot; &quot;Fumbles Fum&quot; &quot;Fumbles Lost&quot; colnames(brady_data_clean) &lt;- paste(colnames(reg_season), reg_season[1, ]) brady_data_clean ## # A tibble: 25 × 23 ## `Year Year` `Team Team` `Games GP` `Games GS` `Games Record` `Passing Cmp` ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Year Team GP GS Record Cmp ## 2 2000 NE 1 0 — 1 ## 3 2001 NE 15 14 11–3 264 ## 4 2002 NE 16 16 9–7 373 ## 5 2003 NE 16 16 14–2 317 ## 6 2004 NE 16 16 14–2 288 ## 7 2005 NE 16 16 10–6 334 ## 8 2006 NE 16 16 12–4 319 ## 9 2007 NE 16 16 16–0 398 ## 10 2008 NE 1 1 1–0 7 ## # ℹ 15 more rows ## # ℹ 17 more variables: `Passing Att` &lt;chr&gt;, `Passing Pct` &lt;chr&gt;, ## # `Passing Yds` &lt;chr&gt;, `Passing Y/A` &lt;chr&gt;, `Passing Lng` &lt;chr&gt;, ## # `Passing TD` &lt;chr&gt;, `Passing Int` &lt;chr&gt;, `Passing Rtg` &lt;chr&gt;, ## # `Rushing Att` &lt;chr&gt;, `Rushing Yds` &lt;chr&gt;, `Rushing Y/A` &lt;chr&gt;, ## # `Rushing Lng` &lt;chr&gt;, `Rushing TD` &lt;chr&gt;, `Sacked Sck` &lt;chr&gt;, ## # `Sacked Yds` &lt;chr&gt;, `Fumbles Fum` &lt;chr&gt;, `Fumbles Lost` &lt;chr&gt; Can we graph Brady’s passing yards over time? First remove line 1 brady_data_clean_types &lt;- brady_data_clean |&gt; slice(-1) |&gt; filter(`Year Year` != &quot;Career&quot;) |&gt; mutate( passing_yards_numeric = as.numeric(str_remove(`Passing Yds`, &quot;,&quot;)), year_numeric = as.numeric(`Year Year`) ) brady_data_clean_types |&gt; ggplot() + geom_col(aes(x = year_numeric, y = passing_yards_numeric)) "],["life-expectancy-and-gdp-from-graphs-to-animation.html", "Chapter 8 Life expectancy and GDP from graphs to animation 8.1 Make the same graph but for three random countries 8.2 Size points by population 8.3 Web scrape region data 8.4 Life expectancy, GDP and Region", " Chapter 8 Life expectancy and GDP from graphs to animation Install required packages #install.packages(&quot;here&quot;) #install.packages(&quot;tidyverse&quot;) #install.packages(&quot;rvest&quot;) #install.packages(c(&quot;gganimate&quot;, &quot;gifski&quot;, &quot;av&quot;)) #install.packages(&quot;treemapify&quot;) Load packages Load Life expectancy data Make a line graph of life expectancy for South Africa This comes in wide-format data life_expectancy |&gt; filter(country == &quot;South Africa&quot;) ## # A tibble: 1 × 302 ## country `1800` `1801` `1802` `1803` `1804` `1805` `1806` `1807` `1808` `1809` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 South A… 33.5 33.5 33.5 33.5 33.5 33.5 33.5 33.5 33.5 33.5 ## # ℹ 291 more variables: `1810` &lt;dbl&gt;, `1811` &lt;dbl&gt;, `1812` &lt;dbl&gt;, `1813` &lt;dbl&gt;, ## # `1814` &lt;dbl&gt;, `1815` &lt;dbl&gt;, `1816` &lt;dbl&gt;, `1817` &lt;dbl&gt;, `1818` &lt;dbl&gt;, ## # `1819` &lt;dbl&gt;, `1820` &lt;dbl&gt;, `1821` &lt;dbl&gt;, `1822` &lt;dbl&gt;, `1823` &lt;dbl&gt;, ## # `1824` &lt;dbl&gt;, `1825` &lt;dbl&gt;, `1826` &lt;dbl&gt;, `1827` &lt;dbl&gt;, `1828` &lt;dbl&gt;, ## # `1829` &lt;dbl&gt;, `1830` &lt;dbl&gt;, `1831` &lt;dbl&gt;, `1832` &lt;dbl&gt;, `1833` &lt;dbl&gt;, ## # `1834` &lt;dbl&gt;, `1835` &lt;dbl&gt;, `1836` &lt;dbl&gt;, `1837` &lt;dbl&gt;, `1838` &lt;dbl&gt;, ## # `1839` &lt;dbl&gt;, `1840` &lt;dbl&gt;, `1841` &lt;dbl&gt;, `1842` &lt;dbl&gt;, `1843` &lt;dbl&gt;, … life_expectancy_sa &lt;- life_expectancy |&gt; filter(country == &quot;South Africa&quot;) |&gt; pivot_longer(-country, names_to = &quot;Year&quot;, values_to = &quot;LifeExpectancy&quot;) |&gt; mutate(Year_Numeric = as.numeric(Year)) life_expectancy_sa ## # A tibble: 301 × 4 ## country Year LifeExpectancy Year_Numeric ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 South Africa 1800 33.5 1800 ## 2 South Africa 1801 33.5 1801 ## 3 South Africa 1802 33.5 1802 ## 4 South Africa 1803 33.5 1803 ## 5 South Africa 1804 33.5 1804 ## 6 South Africa 1805 33.5 1805 ## 7 South Africa 1806 33.5 1806 ## 8 South Africa 1807 33.5 1807 ## 9 South Africa 1808 33.5 1808 ## 10 South Africa 1809 33.5 1809 ## # ℹ 291 more rows graph the data life_expectancy_sa |&gt; ggplot() + geom_line(aes(x = Year_Numeric, y = LifeExpectancy)) 8.1 Make the same graph but for three random countries random3 &lt;- life_expectancy |&gt; select(country) |&gt; sample_n(3) random3 ## # A tibble: 3 × 1 ## country ## &lt;chr&gt; ## 1 Nauru ## 2 Djibouti ## 3 South Sudan life_expectancy_random3 &lt;- life_expectancy |&gt; filter(country %in% random3$country) |&gt; pivot_longer(-country, names_to = &quot;Year&quot;, values_to = &quot;LifeExpectancy&quot;) |&gt; mutate(Year_Numeric = as.numeric(Year)) life_expectancy_random3 ## # A tibble: 903 × 4 ## country Year LifeExpectancy Year_Numeric ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Djibouti 1800 29.9 1800 ## 2 Djibouti 1801 29.9 1801 ## 3 Djibouti 1802 29.9 1802 ## 4 Djibouti 1803 29.9 1803 ## 5 Djibouti 1804 29.9 1804 ## 6 Djibouti 1805 29.9 1805 ## 7 Djibouti 1806 29.9 1806 ## 8 Djibouti 1807 29.9 1807 ## 9 Djibouti 1808 29.9 1808 ## 10 Djibouti 1809 29.9 1809 ## # ℹ 893 more rows life_expectancy_random3 |&gt; ggplot() + geom_line(aes(x = Year_Numeric, y = LifeExpectancy, color=country)) ## Warning: Removed 150 rows containing missing values or values outside the ## scale range (`geom_line()`). Filter for life expectancy of zero. Did we ever complete this inclass? life_expectancy &lt;- life_expectancy |&gt; filter(country %in% random3$country) |&gt; pivot_longer(-country, names_to = &quot;Year&quot;, values_to = &quot;LifeExpectancy&quot;) |&gt; mutate(Year_Numeric = as.numeric(Year)) life_expectancy_random3 gdp &lt;- read_csv(here(&quot;data_public&quot;, &quot;gdp_pcap.csv&quot;)) ## Rows: 195 Columns: 302 ## ── Column specification ───────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (199): country, 1901, 1903, 1905, 1906, 1907, 1908, 1909, 1910, 1911, 19... ## dbl (103): 1800, 1801, 1802, 1803, 1804, 1805, 1806, 1807, 1808, 1809, 1810,... ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. Look out, there are k’s in the data for 1,000! We should replace k with 000 and . with nothing. #gdp_long &lt;- gdp |&gt; # pivot_longer Check out stringr cheat sheet! Test case for 10k str_extract(&quot;10k&quot;, &quot;0123456789&quot;) ## [1] NA as.numeric(str_remove(&quot;10k&quot;, &quot;k&quot;)) * 1000 ## [1] 10000 Try out the conversion testconversion &lt;- gdp |&gt; mutate(`1800` = case_when( str_detect(`1800`, &quot;k&quot;) ~ as.numeric(str_remove(`1800`, &quot;k&quot;)) * 1000, !str_detect(`1800`, &quot;k&quot;) ~ as.numeric(`1800`), str_detect(`1901`, &quot;k&quot;) ~ as.numeric(str_remove(`1901`, &quot;k&quot;)) * 1000, !str_detect(`1901`, &quot;k&quot;) ~ as.numeric(`1901`) )) ## Warning: There was 1 warning in `mutate()`. ## ℹ In argument: `1800 = case_when(...)`. ## Caused by warning: ## ! NAs introduced by coercion Ok, what were those NAs about, and can we find a better way to do this compared to copying and pasting code. Apply a function to many columns simultaneously: gdp_numeric &lt;- gdp |&gt; select(-country) |&gt; mutate_all(~case_when( str_detect(.x, &quot;k&quot;) ~ as.numeric(str_remove(.x, &quot;k&quot;)) * 1000, !str_detect(.x, &quot;k&quot;) ~ as.numeric(.x) )) |&gt; cbind(country = gdp$country) ## Warning: There were 198 warnings in `mutate()`. ## The first warning was: ## ℹ In argument: `1901 = (structure(function (..., .x = ..1, .y = ..2, ## . = ..1) ...`. ## Caused by warning: ## ! NAs introduced by coercion ## ℹ Run `dplyr::last_dplyr_warnings()` to see the 197 remaining ## warnings. na_rows &lt;- gdp_numeric[!complete.cases(gdp_numeric), ] Ok, now pivot gdp_long &lt;- gdp_numeric |&gt; pivot_longer(-country, names_to = &quot;Year&quot;, values_to = &quot;GDP_Per_Capita&quot;) |&gt; mutate(Year = as.numeric(Year)) And graph gdp_long |&gt; filter(country == &quot;Sweden&quot;) |&gt; ggplot() + geom_line(aes(x = Year, y = GDP_Per_Capita)) 8.1.1 Join the life expectancy data together with income data gdp_long needs to be combined with life_expectancy Save the long form of life expectancy life_expectancy_long &lt;- life_expectancy |&gt; pivot_longer(-country, names_to = &quot;Year&quot;, values_to = &quot;LifeExpectancy&quot;) |&gt; mutate(Year = as.numeric(Year)) Now join them! life_gdp &lt;- life_expectancy_long |&gt; left_join(gdp_long, by = c(&quot;country&quot;, &quot;Year&quot;)) Graph a specific country and year life_gdp |&gt; filter(Year == 2000) |&gt; ggplot() + geom_point(aes(x = GDP_Per_Capita, y = LifeExpectancy)) + scale_x_log10() ## Warning: Removed 1 row containing missing values or values outside the scale ## range (`geom_point()`). Load joined data pop_gdp_le &lt;- read_csv(here(&quot;data_public&quot;, &quot;Pop_GDP_LE_data.csv&quot;)) ## Rows: 58695 Columns: 5 ## ── Column specification ───────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (1): country ## dbl (4): Year, GDP_Per_Capita, LifeExpectancy, Population ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 8.2 Size points by population pop_gdp_le |&gt; filter(Year == 2000) |&gt; ggplot() + geom_point(aes(x = GDP_Per_Capita, y = LifeExpectancy, size = Population) ) + scale_x_log10() 8.3 Web scrape region data so that we can color the points by region. Search “country by region plain text” https://statisticstimes.com/geography/countries-by-continents.php Save the table to cSV file write_csv(region_data, here(&quot;data_public&quot;, &quot;region_data.csv&quot;)) Load region table from CSV file region_data &lt;- read_csv(here(&quot;data_public&quot;, &quot;region_data.csv&quot;)) region_data ## # A tibble: 249 × 7 ## No `Country or Area` `ISO-alpha3 Code` `M49 Code` `Region 1` `Region 2` ## &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; ## 1 1 Afghanistan AFG 4 Southern A… &lt;NA&gt; ## 2 2 Åland Islands ALA 248 Northern E… &lt;NA&gt; ## 3 3 Albania ALB 8 Southern E… &lt;NA&gt; ## 4 4 Algeria DZA 12 Northern A… &lt;NA&gt; ## 5 5 American Samoa ASM 16 Polynesia &lt;NA&gt; ## 6 6 Andorra AND 20 Southern E… &lt;NA&gt; ## 7 7 Angola AGO 24 Middle Afr… Sub-Sahar… ## 8 8 Anguilla AIA 660 Caribbean Latin Ame… ## 9 9 Antarctica ATA 10 Antarctica &lt;NA&gt; ## 10 10 Antigua and Barbuda ATG 28 Caribbean Latin Ame… ## # ℹ 239 more rows ## # ℹ 1 more variable: Continent &lt;chr&gt; all_country_data &lt;- pop_gdp_le |&gt; left_join( region_data |&gt; select(`Country or Area`, Continent), by = c(&quot;country&quot; = &quot;Country or Area&quot;)) You can disambiguate functions using ::, e.g. dplyr::select to specify you want the select function specifically from the dplyr package 8.4 Life expectancy, GDP and Region all_country_data |&gt; filter(Year == 2000) |&gt; ggplot() + geom_point( aes( x = GDP_Per_Capita, y = LifeExpectancy, size = Population, color = Continent ) ) + scale_x_log10() There were some missing continent data! Let’s inspect data with missing continent data. all_country_data |&gt; filter(Year == 2000, is.na(Continent)) ## # A tibble: 29 × 6 ## country Year GDP_Per_Capita LifeExpectancy Population Continent ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 UAE 2000 92400 69.1 3280000 &lt;NA&gt; ## 2 Bolivia 2000 5390 66.5 8590000 &lt;NA&gt; ## 3 Brunei 2000 74300 72.5 334000 &lt;NA&gt; ## 4 Cote d&#39;Ivoire 2000 4000 51.1 16800000 &lt;NA&gt; ## 5 Congo, Dem. Rep. 2000 707 53.9 48600000 &lt;NA&gt; ## 6 Congo, Rep. 2000 4580 53 3130000 &lt;NA&gt; ## 7 Cape Verde 2000 4370 70.3 458000 &lt;NA&gt; ## 8 Czech Republic 2000 24900 75.2 10200000 &lt;NA&gt; ## 9 Micronesia, Fed. St… 2000 3450 62.6 112000 &lt;NA&gt; ## 10 UK 2000 39800 78 58900000 &lt;NA&gt; ## # ℹ 19 more rows We are missing a lot of countries because of inconsistent spelling. So, Why not use ISO codes!?! "],["animation.html", "Chapter 9 Animation 9.1 Create the animation 9.2 Rendered Animation 9.3 Animate a bar chart 9.4 Make a tree map", " Chapter 9 Animation Transitions allow you to animate by different variables By default, gganimate makes 100 frames. Themes help style the graphs! Let’s have one code block to tinker with the graph design, and then a second block to tinker with the animation 9.0.1 Graph all years all_country_data |&gt; ggplot() + geom_point(aes(x = GDP_Per_Capita, y = LifeExpectancy, size = Population, color = Continent) ) + scale_x_log10() ## Warning: Removed 2079 rows containing missing values or values outside the ## scale range (`geom_point()`). 9.1 Create the animation animation1 &lt;- all_country_data |&gt; ggplot() + geom_point( aes( x = GDP_Per_Capita, y = LifeExpectancy, size = Population, color = Continent ) ) + geom_text(x = 100000, y = 13, label = &quot;{frame_time}&quot;) + scale_x_log10() + ggtitle(&quot;{frame_time}&quot;) + theme( panel.background = element_rect(fill = &quot;lightblue&quot;, colour = &quot;lightblue&quot;, linewidth = 0.5, linetype = &quot;solid&quot;), panel.grid.major = element_line(size = 0.5, linetype = &#39;solid&#39;, colour = &quot;white&quot;), panel.grid.minor = element_line(size = 0.25, linetype = &#39;solid&#39;, colour = &quot;white&quot;) ) + transition_time(Year) ## Warning: The `size` argument of `element_line()` is deprecated as of ggplot2 ## 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. Render the animation anim_speed &lt;- 3 anim_duration &lt;- (max(all_country_data$Year) - min(all_country_data$Year) + 1) / anim_speed animate(animation1, fps = anim_speed, duration = anim_duration) save the animation anim_save(here(&quot;scripts&quot;, &quot;life_gdp.gif&quot;) 9.2 Rendered Animation Life GDP Animation 9.3 Animate a bar chart bar_animation &lt;- all_country_data |&gt; filter(country %in% c(&quot;Mexico&quot;, &quot;UK&quot;, &quot;Morocco&quot;)) |&gt; ggplot() + geom_col(aes(y = country, x = Population)) + transition_time(Year) animate(bar_animation, nframes = 300) Save bar chart animation anim_save(here(&quot;scripts&quot;, &quot;bar_anim.gif&quot;)) Bar Chart Animation 9.4 Make a tree map all_country_data |&gt; filter(Year == 2000) |&gt; filter(country %in% c(&quot;Mexico&quot;, &quot;UK&quot;, &quot;Morocco&quot;, &quot;France&quot;, &quot;Spain&quot;)) |&gt; ggplot(aes(area = Population, fill = Population, label = paste(country, &quot;\\n&quot;, Population))) + geom_treemap() + geom_treemap_text() + scale_fill_viridis_c() 9.4.1 Animate a tree map tree_animation &lt;- all_country_data |&gt; filter(country %in% c(&quot;Mexico&quot;, &quot;UK&quot;, &quot;Morocco&quot;, &quot;France&quot;, &quot;Spain&quot;)) |&gt; ggplot(aes(area = Population, fill = Population, label = paste(country, &quot;\\n&quot;, Population))) + geom_treemap() + geom_treemap_text() + transition_time(Year) + scale_fill_viridis_c() animate(tree_animation, nframes = 300) Save tree map animation anim_save(here(&quot;scripts&quot;, &quot;tree_anim.gif&quot;)) Bar Chart Animation "],["mapping-in-r.html", "Chapter 10 Mapping in R 10.1 Map alcohol consumption per capita 10.2 Map volcanoes", " Chapter 10 Mapping in R library(tigris) ## To enable caching of data, set `options(tigris_use_cache = TRUE)` ## in your R script or .Rprofile. library(sf) ## Warning: package &#39;sf&#39; was built under R version 4.3.3 ## Linking to GEOS 3.11.2, GDAL 3.8.2, PROJ 9.3.1; sf_use_s2() is TRUE library(rvest) library(leaflet) library(here) library(tidyverse) Load counties of Vermont using Tigris package vermont_counties |&gt; ggplot() + geom_sf(aes(geometry = geometry, fill = AWATER)) Query all states Map all states all_states |&gt; ggplot() + geom_sf(aes(geometry = geometry, fill = AWATER/ALAND)) Shift the geometry of the coordinates and filter out territories all_states |&gt; shift_geometry() |&gt; filter(as.numeric(GEOID) &lt; 60) |&gt; ggplot() + geom_sf(aes(geometry = geometry, fill = AWATER/ALAND)) 10.1 Map alcohol consumption per capita Scrape alcohol data url &lt;- &quot;https://wisevoter.com/state-rankings/alcohol-consumption-by-state/&quot; alcohol &lt;- url |&gt; read_html() |&gt; html_element(&quot;table&quot;) |&gt; html_table() Join alcohol to states all_states_booze &lt;- all_states |&gt; filter(GEOID &lt; 60) |&gt; left_join(alcohol, by = join_by(&quot;NAME&quot; == &quot;State&quot;)) all_states_booze &lt;- all_states_booze |&gt; mutate(alcohol_per_cap = as.numeric(str_remove(`Alcohol Consumption Per Capita`, &quot; gal&quot;))) Make a map! all_states_booze |&gt; shift_geometry() |&gt; filter(as.numeric(GEOID) &lt; 60) |&gt; ggplot() + geom_sf(aes(geometry = geometry, fill = alcohol_per_cap)) + scale_fill_binned() Make an interactive map with Leaflet leaflet() |&gt; addTiles() |&gt; addMarkers(lat= 44.0139, lng = -73.1814, label = &quot;BiHall&quot;, popup = &quot;Here we are&quot;) 10.2 Map volcanoes Load the dataset volcanoes &lt;- read_tsv(here(&quot;data_public&quot;, &quot;volcano_data.tsv&quot;)) ## Rows: 1608 Columns: 11 ## ── Column specification ───────────────────────────────────────────── ## Delimiter: &quot;\\t&quot; ## chr (7): Search Parameters, Volcano Name, Country, Location, Type, Status, L... ## dbl (4): Volcano Number, Latitude, Longitude, Elevation (m) ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. head(volcanoes, 5) ## # A tibble: 5 × 11 ## `Search Parameters` `Volcano Number` `Volcano Name` Country Location Latitude ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 [] NA &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; NA ## 2 &lt;NA&gt; 250010 St. Andrew Str… Papua … Admiral… -2.38 ## 3 &lt;NA&gt; 250020 Baluan United… Admiral… -2.57 ## 4 &lt;NA&gt; 250030 Central Bismar… United… Admiral… -3.03 ## 5 &lt;NA&gt; NA Dacht-I-Navar … Afghan… Afghani… 34.0 ## # ℹ 5 more variables: Longitude &lt;dbl&gt;, `Elevation (m)` &lt;dbl&gt;, Type &lt;chr&gt;, ## # Status &lt;chr&gt;, `Last Known Eruption` &lt;chr&gt; Map the volcanoes volcanoes |&gt; leaflet() |&gt; addTiles() |&gt; addMarkers(lat = ~Latitude, lng = ~Longitude, label = ~`Volcano Name`) Cluster markers volcanoes |&gt; leaflet() |&gt; addTiles() |&gt; addMarkers(lat = ~Latitude, lng = ~Longitude, label = ~`Volcano Name`, clusterOptions = markerClusterOptions()) Circles volcanoes |&gt; leaflet() |&gt; addTiles() |&gt; addCircles(lat = ~Latitude, lng = ~Longitude, label = ~`Volcano Name`, radius = 15000, color = &quot;red&quot;, weight = 1, fillOpacity = 0.8) Map alcohol consumption with leaflet all_states_booze |&gt; filter(GEOID &lt; 60) |&gt; leaflet() |&gt; addPolygons(label = ~NAME) Load India data from a shapefile india_states &lt;- st_read(here(&quot;data_public&quot;, &quot;indiashp&quot;, &quot;Indian_States.shp&quot;)) ## Reading layer `Indian_States&#39; from data source ## `C:\\GitHub\\opengisci\\wt25_josephholler\\data_public\\indiashp\\Indian_States.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 36 features and 1 field ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: 68.18625 ymin: 6.755953 xmax: 97.41529 ymax: 37.07827 ## CRS: NA Map Indian states with ggplot india_states |&gt; ggplot() + geom_sf() + geom_sf_label(aes(label = st_nm)) Map Indian states with leaflet india_states |&gt; leaflet() |&gt; addTiles() |&gt; addPolygons() "],["shiny-apps.html", "Chapter 11 Shiny Apps 11.1 Strange online food data 11.2 Liberal arts college rankings 11.3 Shiny App notes", " Chapter 11 Shiny Apps install.packages(&quot;tidygeocoder&quot;) It is wise to plan out the data and visualizations for a Shiny App in a normal R script. Once you’re satisfied with the data and visualization approaches, transfer them into an app. The app should be self-contained in its own folder for easy transfer to shinyapps.io. Load packages 11.1 Strange online food data Create an initial app in an exploration folder. It’s from Kaggle! Load it in… food &lt;- read_csv(here(&quot;apps&quot;, &quot;exploration&quot;, &quot;onlinefoods.csv&quot;)) ## New names: ## Rows: 388 Columns: 13 ## ── Column specification ## ───────────────────────────────────────────── Delimiter: &quot;,&quot; chr ## (8): Gender, Marital Status, Occupation, Monthly Income, Educational ## Qua... dbl (5): Age, Family size, latitude, longitude, Pin code ## ℹ Use `spec()` to retrieve the full column specification for this ## data. ℹ Specify the column types or set `show_col_types = FALSE` to ## quiet this message. ## • `` -&gt; `...13` Make a bar graph of income and education food |&gt; ggplot() + geom_bar(aes(x = `Monthly Income`, fill = `Educational Qualifications`)) 11.2 Liberal arts college rankings Let’s build a more serious app of Liberal Arts Colleges U.S. News rankings in our liberalarts folder. We can geocode with the tidygeocoder package. examplePlaces &lt;- data.frame( location = c(&quot;Middlebury College&quot;, &quot;Williams College&quot;)) examplePlacesCoded &lt;- examplePlaces |&gt; geocode(location) ## Passing 2 addresses to the Nominatim single address geocoder ## Query completed in: 2 seconds examplePlacesCoded ## # A tibble: 2 × 3 ## location lat long ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Middlebury College 44.0 -73.2 ## 2 Williams College 42.7 -73.2 We can geocode addresses composed of college names and their states. Geocoding takes some time, and often requires an API with a software-as-service subscription. college_data &lt;- read_csv(here(&quot;apps&quot;, &quot;liberalarts&quot;, &quot;usnews.csv&quot;)) college_data_with_locations &lt;- college_data |&gt; mutate(location = paste0(`College Name`, &quot;, &quot;, State)) |&gt; geocode(address = location) Always save geocoding results as a data object and immediately save as a data file. You don’t want to repeat geocoding if at all possible. saveRDS(college_data_with_locations, here(&quot;apps&quot;, &quot;liberalarts&quot;, &quot;LA_CollegesUS.RDS&quot;)) Load geocoded results college_data_with_locations &lt;- readRDS( here(&quot;apps&quot;, &quot;liberalarts&quot;, &quot;LA_CollegesUS.RDS&quot;)) Did everything geocode? college_data_with_locations |&gt; filter(is.na(lat)) ## # A tibble: 27 × 44 ## `College Name` State `IPEDS ID` `2023` `2022` `2021` `2020` `2019` `2018` ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 Franklin and Mars… PA 212577 39 42 43 38 36 39 ## 2 Sewanee-Universit… TN 221519 51 50 47 43 49 41 ## 3 Washington and Je… PA 216667 94 92 96 92 103 106 ## 4 Concordia College… MN 173300 145 141 130 132 127 117 ## 5 Ava Maria Univers… FL 442295 T2 (1… T2 (1… T2 (1… T2 (1… T2 (1… &lt;NA&gt; ## 6 University of Pue… PR 243151 T2 (1… T2 (1… 166 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 7 Albert Magnus Col… CT 128498 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 8 Albertson College ID 142294 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 9 Borromeo College … OH 203368 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 10 Carson-Newman Col… TN 219806 &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## # ℹ 17 more rows ## # ℹ 35 more variables: `2017` &lt;chr&gt;, `2016` &lt;dbl&gt;, `2015` &lt;dbl&gt;, `2014` &lt;chr&gt;, ## # `2013` &lt;chr&gt;, `2012` &lt;chr&gt;, `2011` &lt;dbl&gt;, `2010` &lt;dbl&gt;, `2009` &lt;dbl&gt;, ## # `2008` &lt;chr&gt;, `2007` &lt;chr&gt;, `2006` &lt;chr&gt;, `2005` &lt;chr&gt;, `2004` &lt;chr&gt;, ## # `2003` &lt;chr&gt;, `2002` &lt;dbl&gt;, `2001` &lt;chr&gt;, `2000` &lt;chr&gt;, `1999` &lt;chr&gt;, ## # `1998` &lt;chr&gt;, `1997` &lt;chr&gt;, `1996` &lt;chr&gt;, `1995` &lt;chr&gt;, `1994` &lt;chr&gt;, ## # `1993` &lt;chr&gt;, `1992` &lt;chr&gt;, `1991` &lt;chr&gt;, `1990` &lt;dbl&gt;, `1989` &lt;dbl&gt;, … Apparently not! For example, OSM has Franklin &amp; Marshall College with an ampersand, and it does not find the college with an “and”. Moving on anyway… 11.2.1 Leaflet map of college locations college_data_with_locations |&gt; filter(!is.na(lat)) |&gt; leaflet() |&gt; addTiles() |&gt; addMarkers(label = ~`College Name`, lng = ~long, lat = ~lat) 11.2.2 Graph college ranking over time Let’s build a graph of college ranking over time. As usual, we’ll have to do some data wrangling first, including converting text to numbers and pivoting. While converting text, we need to decide what to do with tied rankings. college_ranks &lt;- college_data_with_locations |&gt; select(-State, -`IPEDS ID`, -lat, -long, -location) |&gt; mutate_all(as.character) |&gt; pivot_longer(-`College Name`, names_to = &quot;Year&quot;, values_to = &quot;Ranking&quot;) |&gt; filter(!is.na(Ranking), Ranking != &quot;?&quot;) |&gt; mutate(rankhigh = case_when(str_detect(Ranking, &quot;\\\\(&quot;) ~ str_extract(Ranking, &quot;(?&lt;=\\\\()[:digit:]+&quot;), .default = Ranking), ranklow = case_when(str_detect(Ranking, &quot;\\\\(&quot;) ~ str_extract(Ranking, &quot;(?&lt;=-)[:digit:]+&quot;), .default = Ranking), rankhigh = as.numeric(rankhigh), ranklow = as.numeric(ranklow), Year = as.numeric(Year)) college_ranks |&gt; saveRDS(here(&quot;apps&quot;, &quot;liberalarts&quot;, &quot;collegeRanks.RDS&quot;)) Filter ranks for a single college selected_college_data &lt;- college_ranks |&gt; filter(`College Name` == &quot;Middlebury College&quot;) selected_college_data ## # A tibble: 36 × 5 ## `College Name` Year Ranking rankhigh ranklow ## &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 Middlebury College 2023 11 11 11 ## 2 Middlebury College 2022 9 9 9 ## 3 Middlebury College 2021 9 9 9 ## 4 Middlebury College 2020 7 7 7 ## 5 Middlebury College 2019 5 5 5 ## 6 Middlebury College 2018 6 6 6 ## 7 Middlebury College 2017 4 4 4 ## 8 Middlebury College 2016 4 4 4 ## 9 Middlebury College 2015 7 7 7 ## 10 Middlebury College 2014 4 4 4 ## # ℹ 26 more rows Now graph the ranks for a single college over time. Notice how the x and y axis limits are set based on the whole dataset, not just the single college. selected_college_data |&gt; ggplot() + geom_line(aes(x = Year, y = rankhigh)) + geom_line(aes(x = Year, y = ranklow)) + ylim(c(max(college_ranks$ranklow), 1)) + xlim(c(min(college_ranks$Year), max(college_ranks$Year))) ### Revisit map to show colleges with ranking data n_rankings &lt;- college_ranks |&gt; count(`College Name`, sort = TRUE) n_rankings ## # A tibble: 356 × 2 ## `College Name` n ## &lt;chr&gt; &lt;int&gt; ## 1 Wheaton College 64 ## 2 St. John&#39;s College 59 ## 3 Westminster College 48 ## 4 Amherst College 38 ## 5 Carleton College 38 ## 6 Haverford College 38 ## 7 Oberlin College and Conservatory 38 ## 8 Pomona College 38 ## 9 Swarthmore College 38 ## 10 Wellesley College 38 ## # ℹ 346 more rows Join number of rankings to college data college_locations_ranks &lt;- college_data_with_locations |&gt; select(`College Name`, State, location, lat, long) |&gt; left_join(n_rankings, by = &quot;College Name&quot;) |&gt; filter(!is.na(lat)) college_locations_ranks |&gt; saveRDS(here(&quot;apps&quot;, &quot;liberalarts&quot;, &quot;collegeLocations.RDS&quot;)) college_locations_ranks |&gt; filter(n &gt;= 5) |&gt; leaflet() |&gt; addTiles() |&gt; addMarkers(label = ~`College Name`, lng = ~long, lat = ~lat) 11.3 Shiny App notes It appears that tmap maps could also be used in Shiny with the tmapOutput() and renderTmap() functions. "],["hurricane-dorian.html", "Chapter 12 Hurricane Dorian 12.1 Temporal analysis 12.2 Text analysis 12.3 Geographic analysis 12.4 Map hurricane tracks with tmap 12.5 Acquire hurricane track data 12.6 Map 2019 North Atlantic storm season 12.7 Map Hurricane Dorian track", " Chapter 12 Hurricane Dorian Install packages install.packages(&quot;tidyverse&quot;) install.packages(&quot;here&quot;) install.packages(&quot;tidytext&quot;) install.packages(&quot;tm&quot;) install.packages(&quot;igraph&quot;) install.packages(&quot;ggraph&quot;) install.packages(&quot;qdapRegex&quot;) Load packages 12.0.1 Load and organize data Check &amp; create private data folder if(!dir.exists(here(&quot;data_private&quot;))) {dir.create(here(&quot;data_private&quot;))} if(!dir.exists(here(&quot;data_public&quot;))) {dir.create(here(&quot;data_public&quot;))} Set up data Create a file path for Dorian data # set file path dorianFile &lt;- here(&quot;data_private&quot;, &quot;dorian_raw.RDS&quot;) # show the file path dorianFile ## [1] &quot;C:/GitHub/opengisci/wt25_josephholler/data_private/dorian_raw.RDS&quot; Check if Dorian data exists locally. If not, download the data. Check if Dorian data exists locally. If not, download the data. if(!file.exists(dorianFile)){ download.file(&quot;https://geography.middlebury.edu/jholler/wt25/dorian/dorian_raw.RDS&quot;, dorianFile, mode=&quot;wb&quot;) } Load Dorian data from RDS file dorian_raw &lt;- readRDS(dorianFile) View Dorian data column names dorian_columns &lt;- colnames(dorian_raw) Save Dorian data column names to a CSV file for documentation and reference write.csv(dorian_columns, here(&quot;data_public&quot;, &quot;dorian_columns.csv&quot;)) 12.1 Temporal analysis What data format is our created_at data using? Try the str function to reveal the structure of a data object. $ picks out a column of a data frame by name str(dorian_raw$created_at) ## POSIXct[1:209387], format: &quot;2019-09-10 19:22:34&quot; &quot;2019-09-10 19:22:34&quot; &quot;2019-09-10 17:14:39&quot; ... Twitter data’s created_at column is in the POSIXct date-time format. Underneath the hood, all date-time data is stored as the number of seconds since January 1, 1970. We wrap that data around functions to transform it into human-readable formats. This format does not record a time zone and uses UTC for its time. Let’s convert that into local (EST) time for the majority of the region of interest using the with_tz function, part of the lubridate package that comes with tidyverse. dorian_est &lt;- dorian_raw |&gt; mutate(created_est = with_tz(created_at, tzone = &quot;EST&quot;)) Try graphing the tweets over time using geom_histogram. Once you get one graph working, keep copying the code into the subsequent block to make improvements. dorian_est |&gt; ggplot() + geom_histogram(aes(x = created_est)) ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. How are the number of bins– and therefore the bin widths– defined? How is the x-axis formatted? 12.1.1 Customize bin widths To control bin widths,geom_histogram accepts a binwidth parameter assuming the units are in seconds. Try setting the bin width to one hour, and create the graph. Try setting the bin width to 6-hour intervals, or even days. dorian_est |&gt; ggplot() + geom_histogram(aes(x = created_est), binwidth = 3600) Reset the bin width to one hour (3600 seconds) before moving on. 12.1.2 Customize a date-time axis To control a date-time x-axis, try adding a scale_x_datetime function. Add a date_breaks parameter with the value \"day\" to label and display grid lines for each day. Add a date_labels parameter to format labels according to codes defined in strptime. Look up documentation with ?strptime. Try \"%b-%d\" for a label with Month abbreviation and day. Add a name parameter to customize the x-axis label. dorian_est |&gt; ggplot() + geom_histogram(aes(x = created_est), binwidth = 3600) + scale_x_datetime(date_breaks = &quot;day&quot;, date_labels = &quot;%b-%d&quot;, name = &quot;time&quot;) Now, try limiting the x-axis to fewer days, e.g. Sept 5 through Sept 8. scale_x_datetime accepts a limits parameter as a list of two dates. You can create an eastern time zone date with as.POSIXct(\"2019-09-05\", tz=\"EST\") dorian_est |&gt; ggplot() + geom_histogram(aes(x = created_est), binwidth = 3600) + scale_x_datetime(date_breaks = &quot;day&quot;, date_labels = &quot;%b-%d&quot;, name = &quot;time&quot;, limits = c(as.POSIXct(&quot;2019-09-06&quot;, tz=&quot;EST&quot;), as.POSIXct(&quot;2019-09-09&quot;, tz=&quot;EST&quot;) ) ) ## Warning: Removed 118438 rows containing non-finite outside the scale range ## (`stat_bin()`). ## Warning: Removed 2 rows containing missing values or values outside the scale ## range (`geom_bar()`). Finally, let’s remove the x-axis limits and switch from geom_histogram to geom_freqpoly dorian_est |&gt; ggplot() + geom_freqpoly(aes(x = created_est), binwidth = 3600) + scale_x_datetime(date_breaks = &quot;day&quot;, date_labels = &quot;%b-%d&quot;, name = &quot;time&quot;) Note that temporal data can be binned into factors using the cut function. look up cut.POSIXt 12.2 Text analysis refer to: https://www.earthdatascience.org/courses/earth-analytics/get-data-using-apis/text-mining-twitter-data-intro-r/ Tokenize the tweet content dorian_words &lt;- dorian_raw |&gt; select(status_id, text) |&gt; mutate(text = rm_hash(rm_tag(rm_twitter_url(tolower(text))))) |&gt; unnest_tokens(input = text, output = word, format = &quot;html&quot;) Remove stop words dorian_words_cleaned &lt;- dorian_words |&gt; filter(!(word %in% stop_words$word)) Remove search terms and junk words junk &lt;- c(&quot;hurricane&quot;, &quot;sharpiegate&quot;, &quot;dorian&quot;, &quot;de&quot;, &quot;https&quot;, &quot;hurricanedorian&quot;, &quot;por&quot;, &quot;el&quot;, &quot;en&quot;, &quot;las&quot;, &quot;del&quot;) dorian_words_cleaned &lt;- dorian_words_cleaned |&gt; filter(!(word %in% junk)) |&gt; filter(!(word %in% stop_words$word)) 12.2.1 Graph word frequencies dorian_words_cleaned %&gt;% count(word, sort = TRUE) %&gt;% head(15) %&gt;% mutate(word = reorder(word, n)) %&gt;% ggplot() + geom_col(aes(x = n, y = word)) + xlab(NULL) + labs( x = &quot;Count&quot;, y = &quot;Unique words&quot;, title = &quot;Count of unique words found in tweets&quot; ) 12.2.2 Graph word associations create n-gram data structure n-grams create pairs of co-located words wrapper function: unnest_ngrams skip_ngrams forms pairs separated by a defined distance It would be keen to remove @ handles here as well. Also consider removing Spanish stop words https://kshaffer.github.io/2017/02/mining-twitter-data-tidy-text-tags/ dorian_text_clean &lt;- dorian_raw |&gt; select(text) |&gt; mutate(text_clean = rm_hash(rm_tag(rm_twitter_url(tolower(text)))), text_clean = removeWords(text_clean, stop_words$word), text_clean = removeWords(text_clean, junk)) Separate and count ngrams dorian_ngrams &lt;- dorian_text_clean |&gt; unnest_tokens(output = paired_words, input = text_clean, token = &quot;ngrams&quot;) |&gt; separate(paired_words, c(&quot;word1&quot;, &quot;word2&quot;), sep = &quot; &quot;) |&gt; count(word1, word2, sort = TRUE) ## Warning: Expected 2 pieces. Additional pieces discarded in 1398438 rows [1, ## 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, ## ...]. Graph ngram word associations # graph a word cloud with space indicating association. # you may change the filter to filter more or less than pairs with 25 instances dorian_ngrams %&gt;% filter(n &gt;= 500 &amp; !is.na(word1) &amp; !is.na(word2)) %&gt;% graph_from_data_frame() %&gt;% ggraph(layout = &quot;fr&quot;) + geom_edge_link(aes(edge_alpha = n, edge_width = n)) + geom_node_point(color = &quot;darkslategray4&quot;, size = 3) + geom_node_text(aes(label = name), vjust = 1.8, size = 3) + labs( title = &quot;Hurricane Dorian Word Associations&quot;, x = &quot;&quot;, y = &quot;&quot; ) + theme( plot.background = element_rect( fill = &quot;grey95&quot;, colour = &quot;black&quot;, size = 1 ), legend.background = element_rect(fill = &quot;grey95&quot;) ) ## Warning: The `size` argument of `element_rect()` is deprecated as of ggplot2 ## 3.4.0. ## ℹ Please use the `linewidth` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. ## Warning: The `trans` argument of `continuous_scale()` is deprecated as of ## ggplot2 3.5.0. ## ℹ Please use the `transform` argument instead. ## This warning is displayed once every 8 hours. ## Call `lifecycle::last_lifecycle_warnings()` to see where this ## warning was generated. 12.3 Geographic analysis Install and load packages for spatial analysis to the beginning of your markdown document, including: sf for simple (geographic vector) features tmap for thematic mapping tidycensus for acquiring census data install.packages(&quot;sf&quot;) install.packages(&quot;tidycensus&quot;) Load packages dorianFile &lt;- here(&quot;data_private&quot;, &quot;dorian_raw.RDS&quot;) dorian_raw &lt;- readRDS(dorianFile) 12.3.1 Construct geographic points from tweets Before conducting geographic analysis, we need to make geographic points for those Tweets with enough location data to do so. Our end goal will be to map Twitter activity by county, so any source of location data precise enough to pinpoint a user’s county will suffice. The three primary sources of geographic information in tweets are: - Geographic coordinates from location services - Place names that users tag to a specific tweet - Location description in users’ profiles 12.3.2 Location services The most precise geographic data is the GPS coordinates from a device with location services, stored in the geo_coords column, which contains a vector of two decimal numbers. In data science, unnest refers to actions in which when complex content from a single column is parsed out or divided out into numerous columns and/or rows. We can use unnest_wider to spread the list content of one column into multiple columns. The challenge is figuring out what the new unique column names will be so that the data frame still functions. In this case, the list items inside geo_coords do not have names. We must therefore use the names_sep argument to add a separator and suffix number to the resulting columns, producing the names geo_coords_1 and geo_coords_2 dorian_geo &lt;- dorian_raw |&gt; unnest_wider(geo_coords, names_sep = &quot;_&quot;) We know that most of the geo_coords data is NA because Twitter users had location services disabled in their privacy settings. How many Tweets had location services enabled, and thus not NA values for geographic coordinates? Answer with the next code block. dorian_geo |&gt; filter(!is.na(geo_coords_1)) |&gt; count() ## # A tibble: 1 × 1 ## n ## &lt;int&gt; ## 1 1274 We know that geographic coordinates in North America have negative x longitude values (west) and positive y latitude values (north) when the coordinates are expressed in decimal degrees. Discern which geo_coords column is which by sampling a 5 of them in the code block below. Hint: select can select a subset of columns using many helper functions, including starts_with to pick out columns that begin with the same prefix. dorian_geo |&gt; filter(!is.na(geo_coords_1)) |&gt; select(starts_with(&quot;geo&quot;)) |&gt; sample_n(5) ## # A tibble: 5 × 2 ## geo_coords_1 geo_coords_2 ## &lt;dbl&gt; &lt;dbl&gt; ## 1 35.6 -78.7 ## 2 39.3 -74.6 ## 3 32.8 -79.9 ## 4 26.3 -80.1 ## 5 34.2 -77.9 Replace ??? below with the correct column suffix. The x coordinate (longitude) is stored in column geo_coords_??? The y coordinate (latitude) is stored in column geo_Coords_??? 12.3.3 Place name Many users also record a place name for their Tweet, but how precise are those place names? Let’s summarize place types count(dorian_raw, place_type) ## # A tibble: 6 × 2 ## place_type n ## &lt;chr&gt; &lt;int&gt; ## 1 admin 2893 ## 2 city 10462 ## 3 country 346 ## 4 neighborhood 43 ## 5 poi 441 ## 6 &lt;NA&gt; 195202 And graph a bar chart of place types dorian_raw |&gt; filter(!is.na(place_type)) |&gt; count(place_type) |&gt; ggplot() + geom_col(aes(x = n, y = place_type)) Twitter then translated the place name into a bounding box with four coordinate pairs, stored as a list of eight decimal numbers in bbox_coords. We will need to find the center of the bounding box by averaging the x coordinates and averaging the y coordinates. In the code block below, convert the single bbox_coords column into one column for each coordinate. dorian_geo_place &lt;- dorian_geo |&gt; unnest_wider(bbox_coords, names_sep = &quot;_&quot;) Sample 5 of the resulting bbox_coords columns to determine which columns may be averaged to find the center of the bounding box. dorian_geo_place |&gt; filter(!is.na(bbox_coords_1)) |&gt; select(starts_with(&quot;bbox&quot;)) |&gt; sample_n(5) ## # A tibble: 5 × 8 ## bbox_coords_1 bbox_coords_2 bbox_coords_3 bbox_coords_4 bbox_coords_5 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 -82.5 -82.4 -82.4 -82.5 27.1 ## 2 -81.3 -81.2 -81.2 -81.3 29.0 ## 3 -83.7 -83.5 -83.5 -83.7 41.6 ## 4 -81.3 -81.1 -81.1 -81.3 34.0 ## 5 -95.7 -95.6 -95.6 -95.7 29.5 ## # ℹ 3 more variables: bbox_coords_6 &lt;dbl&gt;, bbox_coords_7 &lt;dbl&gt;, ## # bbox_coords_8 &lt;dbl&gt; The minimum x coordinate (longitude) is stored in column bbox_coords_??? The maximum x coordinate (longitude) is stored in column bbox_coords_??? The minimum y coordinate (latitude) is stored in column bbox_coords_??? The maximum y coordinate (latitude) is stored in column bbox_coords_??? For only place names at least as precise as counties (poi, city, or neighborhood), calculate the center coordinates in columns named bbox_meanx and bbox_meany. It will be easier to find averages using simple math than to use rowwise and c_across for row-wise aggregate functions. good_places &lt;- c(&quot;poi&quot;, &quot;city&quot;, &quot;neighborhood&quot;) dorian_geo_place &lt;- dorian_geo_place |&gt; mutate(bbox_meanx = case_when(place_type %in% good_places ~ (bbox_coords_1 + bbox_coords_2) / 2), bbox_meany = case_when(place_type %in% good_places ~ (bbox_coords_6 + bbox_coords_7) / 2) ) Sample 5 rows of the place type column and columns starting with bbox for tweets with non-NA place types to see whether the code above worked as expected. dorian_geo_place |&gt; filter(!is.na(place_type)) |&gt; select(place_type, starts_with(&quot;bbox&quot;)) |&gt; sample_n(5) ## # A tibble: 5 × 11 ## place_type bbox_coords_1 bbox_coords_2 bbox_coords_3 bbox_coords_4 ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 city -80.6 -80.4 -80.4 -80.6 ## 2 city -74.3 -74.1 -74.1 -74.3 ## 3 city -79.1 -78.9 -78.9 -79.1 ## 4 admin -83.4 -78.5 -78.5 -83.4 ## 5 city -85.9 -85.8 -85.8 -85.9 ## # ℹ 6 more variables: bbox_coords_5 &lt;dbl&gt;, bbox_coords_6 &lt;dbl&gt;, ## # bbox_coords_7 &lt;dbl&gt;, bbox_coords_8 &lt;dbl&gt;, bbox_meanx &lt;dbl&gt;, ## # bbox_meany &lt;dbl&gt; To be absolutely sure, find the aggregate mean of bbox_meanx and bbox_meany by place_type dorian_geo_place |&gt; group_by(place_type) |&gt; summarize(mean(bbox_meanx), mean(bbox_meany)) ## # A tibble: 6 × 3 ## place_type `mean(bbox_meanx)` `mean(bbox_meany)` ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 admin NA NA ## 2 city -80.9 35.7 ## 3 country NA NA ## 4 neighborhood -81.9 34.4 ## 5 poi -78.8 34.7 ## 6 &lt;NA&gt; NA NA Finally, create xlng (longitude) and ylat (latitude) columns and populate them with coordinates, using geographic coordinates first and filling in with place coordinates second. Hint: if none of the cases are TRUE in a case_when function, then .default = can be used at the end to assign a default value. dorian_geo_place &lt;- dorian_geo_place |&gt; mutate(xlng = case_when(!is.na(geo_coords_2) ~ geo_coords_2, .default = bbox_meanx), ylat = case_when(!is.na(geo_coords_1) ~ geo_coords_1, .default = bbox_meany) ) Sample the xlng and ylat data along side the geographic coordinates and bounding box coordinates to verify. Re-run the code block until you find at least some records with geographic coordinates. dorian_geo_place |&gt; filter(!is.na(xlng)) |&gt; select(xlng, ylat, geo_coords_1, geo_coords_2, bbox_meanx, bbox_meany, place_type) |&gt; sample_n(5) ## # A tibble: 5 × 7 ## xlng ylat geo_coords_1 geo_coords_2 bbox_meanx bbox_meany place_type ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 -79.3 43.6 NA NA -79.3 43.6 city ## 2 -81.6 41.5 NA NA -81.6 41.5 city ## 3 -77.0 38.9 NA NA -77.0 38.9 city ## 4 -76.9 34.9 NA NA -76.9 34.9 city ## 5 -71.5 41.8 NA NA -71.5 41.8 city 12.3.4 Geographic point data The sf pacakge allows us to construct geographic data according to open GIS standards, making tidyverse dataframes into geodataframes analagous to geopackages and shapefiles. Select the status ID, created date-time, x and y coordinate columns. Filter the data for only those Tweets with geographic coordinates. Then, use the st_as_sf function to create geographic point data. A crs is a coordinate reference system, and Geographic coordinates are generally stored in the system numbered 4326 (WGS 1984 geographic coordinates). tweet_points &lt;- dorian_geo_place |&gt; select(status_id, created_at, xlng, ylat) |&gt; filter(!is.na(xlng)) |&gt; st_as_sf(coords = c(&quot;xlng&quot;, &quot;ylat&quot;), crs = 4326) Graph (map) the tweets! tweet_points |&gt; ggplot() + geom_sf(alpha=0.01) 12.3.5 Aquire Census data For details on the variables, see https://api.census.gov/data/2020/dec/dp/groups/DP1.html countyFile &lt;- here(&quot;data_public&quot;, &quot;counties.RDS&quot;) censusVariablesFile &lt;- here(&quot;data_public&quot;, &quot;census_variables.csv&quot;) if(!file.exists(countyFile)){ counties &lt;- get_decennial( geography = &quot;county&quot;, table = &quot;DP1&quot;, sumfile = &quot;dp&quot;, output = &quot;wide&quot;, geometry = TRUE, keep_geo_vars = TRUE, year = 2020 ) saveRDS(counties, countyFile) census_variables &lt;- load_variables(2020, dataset=&quot;dp&quot;) write_csv(census_variables, censusVariablesFile) } else{ counties &lt;- readRDS(countyFile) census_variables &lt;- read_csv(censusVariablesFile) } ## Rows: 320 Columns: 3 ## ── Column specification ───────────────────────────────────────────── ## Delimiter: &quot;,&quot; ## chr (3): name, label, concept ## ## ℹ Use `spec()` to retrieve the full column specification for this data. ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message. 12.3.6 Select counties of interest select only the states you want, with FIPS state codes look up fips codes here: https://en.wikipedia.org/wiki/Federal_Information_Processing_Standard_state_code counties &lt;- filter( counties, STATEFP %in% c( &quot;54&quot;, &quot;51&quot;, &quot;50&quot;, &quot;47&quot;, &quot;45&quot;, &quot;44&quot;, &quot;42&quot;, &quot;39&quot;, &quot;37&quot;, &quot;36&quot;, &quot;05&quot;, &quot;01&quot;, &quot;34&quot;, &quot;33&quot;, &quot;29&quot;, &quot;28&quot;, &quot;25&quot;, &quot;24&quot;, &quot;23&quot;, &quot;22&quot;, &quot;21&quot;, &quot;18&quot;, &quot;17&quot;, &quot;13&quot;, &quot;12&quot;, &quot;11&quot;, &quot;10&quot;, &quot;09&quot; ) ) # saveRDS(counties, here(&quot;data_public&quot;, &quot;counties_east.RDS&quot;)) 12.3.7 Map population density and tweet points Calculate population density in terms of people per square kilometer of land. DP1_0001C is the total population ALAND is the area of land in square meters counties &lt;- mutate(counties, popdensity = DP1_0001C / (ALAND / 1000000)) Map population density We can use ggplot, but rather than the geom_ functions we have seen before for the data geometry, use geom_sf for our simple features geographic data. cut_interval is an equal interval classifier, while cut_number is a quantile / equal count classifier function. Try switching between the two and regraphing the map below. ggplot() + geom_sf(data = counties, aes(fill = cut_number(popdensity, 5)), color = &quot;grey&quot;) + scale_fill_brewer(palette = &quot;GnBu&quot;) + guides(fill = guide_legend(title = &quot;Population Density&quot;)) Add twitter points to the map as a second geom_sf data layer. ggplot() + geom_sf(data = counties, aes(fill = cut_number(popdensity, 5)), color = &quot;grey&quot;) + scale_fill_brewer(palette = &quot;GnBu&quot;) + guides(fill = guide_legend(title = &quot;Population Density&quot;)) + geom_sf(data = tweet_points, colour = &quot;purple&quot;, alpha = 0.05, size = 1 ) + labs(title = &quot;Tweet Locations After Hurricane Dorian&quot;) 12.3.8 Join tweets to counties Spatial functions for vector geographies almost universally start with st_ Twitter data uses a global geographic coordinate system (WGS 1984, with code 4326) Census data uses North American geographic coordinates (NAD 1987, with code 4269) Before making geographic comparisons between datasets, everything should be stored in the same coordinate reference system (crs). st_crs checks the coordinate reference system of geographic data st_transform mathematically translates coordinates from one system to another. Transform Tweet points into the North American coordinate system and then check their CRS. tweet_points &lt;- st_transform(tweet_points, 4269) st_crs(tweet_points) ## Coordinate Reference System: ## User input: EPSG:4269 ## wkt: ## GEOGCRS[&quot;NAD83&quot;, ## DATUM[&quot;North American Datum 1983&quot;, ## ELLIPSOID[&quot;GRS 1980&quot;,6378137,298.257222101, ## LENGTHUNIT[&quot;metre&quot;,1]]], ## PRIMEM[&quot;Greenwich&quot;,0, ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## CS[ellipsoidal,2], ## AXIS[&quot;geodetic latitude (Lat)&quot;,north, ## ORDER[1], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## AXIS[&quot;geodetic longitude (Lon)&quot;,east, ## ORDER[2], ## ANGLEUNIT[&quot;degree&quot;,0.0174532925199433]], ## USAGE[ ## SCOPE[&quot;Geodesy.&quot;], ## AREA[&quot;North America - onshore and offshore: Canada - Alberta; British Columbia; Manitoba; New Brunswick; Newfoundland and Labrador; Northwest Territories; Nova Scotia; Nunavut; Ontario; Prince Edward Island; Quebec; Saskatchewan; Yukon. Puerto Rico. United States (USA) - Alabama; Alaska; Arizona; Arkansas; California; Colorado; Connecticut; Delaware; Florida; Georgia; Hawaii; Idaho; Illinois; Indiana; Iowa; Kansas; Kentucky; Louisiana; Maine; Maryland; Massachusetts; Michigan; Minnesota; Mississippi; Missouri; Montana; Nebraska; Nevada; New Hampshire; New Jersey; New Mexico; New York; North Carolina; North Dakota; Ohio; Oklahoma; Oregon; Pennsylvania; Rhode Island; South Carolina; South Dakota; Tennessee; Texas; Utah; Vermont; Virginia; Washington; West Virginia; Wisconsin; Wyoming. US Virgin Islands. British Virgin Islands.&quot;], ## BBOX[14.92,167.65,86.45,-40.73]], ## ID[&quot;EPSG&quot;,4269]] Here, we want to map the number, density, or ratio of tweets by county. Later on, we may want to filter tweets by their location, or which county and state they are in. For both goals, the first step is to join information about counties to individual tweets. You’ve already seen data joined by column attributes (GDP per capita and life expectancy by country and year) using left_join. Data can also be joined by geographic location with a spatial version, st_join. If you look up the default parameters of st_join, it will be a left join with the join criteria st_intersects. But first, which columns from counties should we join to tweet_points, if we ultimately want to re-attach tweet data to counties and we might want to filter tweets by state or by state and by county? Hint for those unfamiliar with Census data, the Census uses GEOIDs to uniquely identify geographic features, including counties. County names are not unique: some states have counties of the same name. Select the GEOID and useful county and state names from counties, and join this information to tweet points by geographic location. In the select below, = is used to rename columns as they are selected. Display a sample of the results and save these useful results as an RDS file. counties_select_columns &lt;- counties |&gt; select(GEOID, county = NAME.x, state = STATE_NAME) tweet_points &lt;- tweet_points |&gt; st_join(counties_select_columns) tweet_points |&gt; sample_n(5) ## Simple feature collection with 5 features and 5 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -82.76477 ymin: 36.84176 xmax: -69.82584 ymax: 43.90888 ## Geodetic CRS: NAD83 ## # A tibble: 5 × 6 ## status_id created_at geometry GEOID county state ## * &lt;chr&gt; &lt;dttm&gt; &lt;POINT [°]&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 117055457755… 2019-09-08 04:28:53 (-69.82584 43.90888) 23023 Sagadahoc Maine ## 2 117013477001… 2019-09-07 00:40:43 (-82.76477 39.97482) 39089 Licking Ohio ## 3 116967038334… 2019-09-05 17:55:25 (-76.35592 36.84176) 51740 Portsmouth Virgi… ## 4 116997307936… 2019-09-06 13:58:13 (-79.27257 43.62931) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; ## 5 117002116673… 2019-09-06 17:09:18 (-79.27257 43.62931) &lt;NA&gt; &lt;NA&gt; &lt;NA&gt; saveRDS(tweet_points, here(&quot;data_public&quot;, &quot;tweet_locations.RDS&quot;)) Pro Tips: If you join data more than once (e.g. by re-running the same code block), R will create duplicate field names and start adding suffixes to them (e.g. .x, .y). If this happens to you, go back and recreate the data frame and then join only once. Once a data frame is geographic, the geometry column is sticky: it stays with the data even if it is not explicitly selected. The only way to be rid of it is with a special st_drop_geometry() function. Fun fact: you cannot join two data frames by attribute (e.g. through left_join()) if they both have geometries. Now, count the number of tweets per county and remove the geometry column as you do so. tweet_counts &lt;- tweet_points |&gt; st_drop_geometry() |&gt; count(GEOID) tweet_counts ## # A tibble: 602 × 2 ## GEOID n ## &lt;chr&gt; &lt;int&gt; ## 1 01001 1 ## 2 01003 5 ## 3 01007 2 ## 4 01015 6 ## 5 01017 1 ## 6 01021 1 ## 7 01043 1 ## 8 01045 1 ## 9 01047 1 ## 10 01051 3 ## # ℹ 592 more rows Join the count of tweets to counties and save counties with this data. counties_tweet_counts &lt;- counties |&gt; left_join(tweet_counts, by = &quot;GEOID&quot;) counties_tweet_counts |&gt; select(-starts_with(c(&quot;DP1&quot;, &quot;A&quot;)), -ends_with(&quot;FP&quot;)) |&gt; sample_n(5) ## Simple feature collection with 5 features and 10 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -88.36278 ymin: 32.5842 xmax: -78.03319 ymax: 41.37651 ## Geodetic CRS: NAD83 ## # A tibble: 5 × 11 ## COUNTYNS GEOID NAME.x NAMELSAD STUSPS STATE_NAME LSAD NAME.y ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 01480124 51069 Frederick Frederick County VA Virginia 06 Frederick… ## 2 00161554 01057 Fayette Fayette County AL Alabama 06 Fayette C… ## 3 01213674 42065 Jefferson Jefferson County PA Pennsylvania 06 Jefferson… ## 4 00424241 17079 Jasper Jasper County IL Illinois 06 Jasper Co… ## 5 00346824 13319 Wilkinson Wilkinson County GA Georgia 06 Wilkinson… ## # ℹ 3 more variables: geometry &lt;MULTIPOLYGON [°]&gt;, popdensity &lt;dbl&gt;, n &lt;int&gt; 12.3.9 Calculate county tweet rates Calculate the tweet_rate as tweets per person. Many counties remain NA because they had no tweets. Rather than missing data, this actually means there are zero tweets. Therefore, convert NA to 0 using replace_na() before calculating the tweet rate. counties_tweet_counts &lt;- counties_tweet_counts |&gt; mutate( n = replace_na(n, 0), tweet_rate = n / DP1_0001C * 10000 ) counties_tweet_counts |&gt; select(-starts_with(c(&quot;DP1&quot;, &quot;A&quot;)), -ends_with(&quot;FP&quot;)) |&gt; sample_n(5) ## Simple feature collection with 5 features and 11 fields ## Geometry type: MULTIPOLYGON ## Dimension: XY ## Bounding box: xmin: -94.61409 ymin: 34.91246 xmax: -82.2613 ymax: 38.8525 ## Geodetic CRS: NAD83 ## # A tibble: 5 × 12 ## COUNTYNS GEOID NAME.x NAMELSAD STUSPS STATE_NAME LSAD NAME.y ## * &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 00424252 17101 Lawrence Lawrence County IL Illinois 06 Lawrenc… ## 2 00069907 05147 Woodruff Woodruff County AR Arkansas 06 Woodruf… ## 3 00516902 21111 Jefferson Jefferson County KY Kentucky 06 Jeffers… ## 4 00758461 29013 Bates Bates County MO Missouri 06 Bates C… ## 5 01008562 37089 Henderson Henderson County NC North Carolina 06 Henders… ## # ℹ 4 more variables: geometry &lt;MULTIPOLYGON [°]&gt;, popdensity &lt;dbl&gt;, n &lt;int&gt;, ## # tweet_rate &lt;dbl&gt; Show a table of the county name and tweet rate for the 10 counties with the highest rates counties_tweet_counts |&gt; arrange(desc(tweet_rate)) |&gt; select(NAME.y, tweet_rate) |&gt; st_drop_geometry() |&gt; head(10) ## # A tibble: 10 × 2 ## NAME.y tweet_rate ## &lt;chr&gt; &lt;dbl&gt; ## 1 Dare County, North Carolina 12.7 ## 2 Carteret County, North Carolina 11.2 ## 3 Nantucket County, Massachusetts 11.2 ## 4 Hyde County, North Carolina 8.72 ## 5 Georgetown County, South Carolina 7.26 ## 6 Charleston County, South Carolina 6.52 ## 7 Horry County, South Carolina 5.95 ## 8 Brunswick County, North Carolina 5.93 ## 9 New Hanover County, North Carolina 5.58 ## 10 Worcester County, Maryland 5.53 Should we multiply the tweet rate by a constant, so that it can be interpreted as, e.g. tweets per 100 people? What constant makes the best sense? Adjust and re-run the rate calculation code block above and note the rate you have chosen. Save the counties data with Twitter counts and rates saveRDS(counties_tweet_counts, here(&quot;data_public&quot;, &quot;counties_tweet_counts.RDS&quot;)) 12.3.10 Map tweet rates by county Make a choropleth map of the tweet rates! ggplot() + geom_sf(data = counties_tweet_counts, aes(fill = cut_interval(tweet_rate, 7)), color = &quot;grey&quot;) + scale_fill_brewer(palette = &quot;GnBu&quot;) + guides(fill = guide_legend(title = &quot;Tweets per 10,000 People&quot;)) 12.3.11 Conclusions Many things here could still be improved upon! We could… Use regular expressions to filter URLs, user handles, and other weird content out of Tweet Content. Combine state names into single words for analysis, e.g. NewYork, NorthCarolina using str_replace Calculate sentiment of each tweet Draw word clouds in place of bar charts of the top most frequent words Geocode profile location descriptions in order to map more content Animate our map of tweets or our graphs of word frequencies by time Switch from using ggplot for cartography to using tmap For the second assignment, by afternoon class on Tuesday: Complete and knit this Rmd To the conclusions, add two or three ideas about improvements or elements of interactivity that we could add to this analysis. Research and try to implement any one of the improvements suggested. This part will be evaluated more based on effort than on degree of success: try something out for 1-2 hours and document the resources you used and what you learned, whether it ultimately worked or not. 12.4 Map hurricane tracks with tmap Install new spatial packages install.packages(&quot;spdep&quot;) install.packages(&quot;tmap&quot;) Load packages Let’s use the tmap package for thematic mapping. https://r-tmap.github.io/tmap/ Note that tmap is rolling out a new version, so some of the online documentation is ahead of the package installed by default. 12.5 Acquire hurricane track data First, download NA (North Atlantic) point and line shapefiles from: https://www.ncei.noaa.gov/products/international-best-track-archive (go to the shapefiles data: https://www.ncei.noaa.gov/data/international-best-track-archive-for-climate-stewardship-ibtracs/v04r01/access/shapefile/ , and find the two files with “NA” for North America: points and lines) Unzip the contents (all 4 files in each zip) into your data_private folder. Read in the lines data hurricaneTracks &lt;- st_read(here(&quot;data_private&quot;, &quot;IBTrACS.NA.list.v04r01.lines.shp&quot;)) ## Reading layer `IBTrACS.NA.list.v04r01.lines&#39; from data source ## `C:\\GitHub\\opengisci\\wt25_josephholler\\data_private\\IBTrACS.NA.list.v04r01.lines.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 124915 features and 179 fields ## Geometry type: LINESTRING ## Dimension: XY ## Bounding box: xmin: -136.9 ymin: 7 xmax: 63 ymax: 83 ## Geodetic CRS: WGS 84 Filter for 2019, and for Dorian specifically. tracks2019 &lt;- hurricaneTracks |&gt; filter(SEASON == 2019) dorianTrack &lt;- tracks2019 |&gt; filter(NAME == &quot;DORIAN&quot;) Load hurricane points hurricanePoints &lt;- st_read(here(&quot;data_private&quot;, &quot;IBTrACS.NA.list.v04r01.points.shp&quot;)) ## Reading layer `IBTrACS.NA.list.v04r01.points&#39; from data source ## `C:\\GitHub\\opengisci\\wt25_josephholler\\data_private\\IBTrACS.NA.list.v04r01.points.shp&#39; ## using driver `ESRI Shapefile&#39; ## Simple feature collection with 127219 features and 179 fields ## Geometry type: POINT ## Dimension: XY ## Bounding box: xmin: -136.9 ymin: 7 xmax: 63 ymax: 83 ## Geodetic CRS: WGS 84 Filter for Dorian in 2019 dorianPoints &lt;- hurricanePoints |&gt; filter(SEASON == 2019, NAME == &quot;DORIAN&quot;) 12.6 Map 2019 North Atlantic storm season The view mode produces an interactive Leaflet map. tmap_mode(mode = &quot;view&quot;) tm_shape(tracks2019) + tm_lines(col = &quot;NAME&quot;, lwd = 3) + tm_shape(dorianTrack) + tm_lines(col = &quot;red&quot;, lwd = &quot;USA_WIND&quot;) 12.7 Map Hurricane Dorian track The plot mode produces a static map. The width of lines can represent the wind speed. tmap_mode(mode = &quot;plot&quot;) ## tmap mode set to plotting tm_shape(dorianTrack) + tm_lines( lwd = &quot;USA_WIND&quot;, scale = 20) ## Legend labels were too wide. Therefore, legend.text.size has been set to 0.64. Increase legend.width (argument of tm_layout) to make the legend wider and therefore the labels larger. The size of bubbles can also represent wind speed. We can produce a more legible map by filtering for one point every 12 hours. dorianPoints &lt;- dorianPoints |&gt; mutate(hour = hour(as.POSIXlt(ISO_TIME))) dorian6hrPts &lt;- dorianPoints |&gt; filter(hour %% 12 == 0) tmap_mode(mode = &quot;plot&quot;) tm_shape(dorian6hrPts) + tm_bubbles(&quot;USA_WIND&quot;, col = &quot;USA_WIND&quot;, alpha = 0.5) "],["spatial-cluster-analysis.html", "Chapter 13 Spatial Cluster Analysis 13.1 Advice for saving and loading data in R 13.2 Load and map county-level tweet rate 13.3 Create Spatial Weight Matrix 13.4 Calculate Getis-Ord G* Statistic 13.5 Map Hotpots 13.6 Remove counties with no tweets", " Chapter 13 Spatial Cluster Analysis 13.1 Advice for saving and loading data in R At the end of the previous lesson, we output an RDS file of census counties with tweet counts joined. In general, it makes sense to output a permanent copy of a data frame when: - you first download, scrape or create the data, as an unedited original copy - you process large or confidential data to the point when it can be shared - you complete a lengthy processing task - you reach a natural break in the analysis where it would be convenient to restart with or iterate with the data in a particular state - you finish the study An advantage of RDS files is that the data objects can be assigned any name as they are read in. Here, we can just call them counties for convenience. 13.2 Load and map county-level tweet rate counties &lt;- readRDS(here(&quot;data_public&quot;, &quot;counties_tweet_counts.RDS&quot;)) Map tweet rate by county using tmap tm_shape(counties) + tm_polygons(&quot;tweet_rate&quot;, border.col = &quot;grey95&quot;, border.alpha = 0.1, palette = &quot;YlOrRd&quot;, n = 5, style = &quot;headtails&quot;, title = &quot;Tweet Rate&quot;) 13.3 Create Spatial Weight Matrix Use 110km Euclidean distance and include self in the weight matrix county_coords &lt;- counties %&gt;% st_centroid() %&gt;% # convert polygons to centroid points st_coordinates() # convert to simple x,y coordinates to play with stdep thresdist &lt;- county_coords %&gt;% dnearneigh(0, 110, longlat = TRUE) %&gt;% # use geodesic distance of 110km # distance should be long enough for every feature to have &gt;= one neighbor include.self() # include a county in its own neighborhood (for G*) thresdist # view statistical summary of the nearest neighbors ## Neighbour list object: ## Number of regions: 1705 ## Number of nonzero links: 48795 ## Percentage nonzero weights: 1.67852 ## Average number of links: 28.61877 Optionally, plot the spatial weight matrix results This should result in a very dense graph, because each county is connected to all other counties within 110 km. plot(thresdist, county_coords, lwd=0.01) # plot nearest neighbor ties 13.4 Calculate Getis-Ord G* Statistic # Create weight matrix from the neighbor objects dwm &lt;- nb2listw(thresdist, zero.policy = T) ######## Local G* Hotspot Analysis ######## # Get Ord G* statistic for hot and cold spots counties$locG &lt;- as.vector(localG(counties$tweet_rate, listw = dwm, zero.policy = TRUE )) # may be same as: # counties$dorrate %&gt;% localG(listw = dwm, zero.policy = TRUE) %&gt;% as.vector() # check summary statistics of the local G score summary(counties$locG) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.9221 -1.1916 -0.8742 -0.2236 -0.1173 13.9514 13.5 Map Hotpots classify G scores by significance values typical of Z-scores where 1.15 is at the 0.125 confidence level, and 1.95 is at the 0.05 confidence level for two tailed z-scores based on Getis and Ord (1995) Doi: 10.1111/j.1538-4632.1992.tb00261.x to find other critical values, use the qnorm() function as shown here: https://methodenlehre.github.io/SGSCLM-R-course/statistical-distributions.html Getis Ord also suggest applying a Bonferroni correction breaks and colors from http://michaelminn.net/tutorials/r-point-analysis/ based on 1.96 as the 95% confidence interval for z-scores if your results don’t have values in each of the 5 categories, you may need to change the values &amp; labels accordingly. What are significance levels and p-values? Read here: https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/what-is-a-z-score-what-is-a-p-value.htm Classify by significance levels siglevel &lt;- c(1.15, 1.95) counties &lt;- counties %&gt;% mutate(sig = cut(locG, c( min(counties$locG), siglevel[2] * -1, siglevel[1] * -1, siglevel[1], siglevel[2], max(counties$locG) ))) rm(siglevel) Map results! # map results! ggplot() + geom_sf(data = counties, aes(fill = sig), color = &quot;white&quot;, lwd = 0.1) + scale_fill_manual( values = c(&quot;#0000FF80&quot;, &quot;#8080FF80&quot;, &quot;#FFFFFF80&quot;, &quot;#FF808080&quot;, &quot;#FF000080&quot;), labels = c(&quot;low&quot;, &quot;&quot;, &quot;insignificant&quot;, &quot;&quot;, &quot;high&quot;), aesthetics = &quot;fill&quot; ) + guides(fill = guide_legend(title = &quot;Hot Spots&quot;)) + labs(title = &quot;Clusters of Hurricane Dorian Twitter Activity&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.title.x = element_blank(), axis.title.y = element_blank() ) 13.6 Remove counties with no tweets Let’s experiment with this hotspot / cluster detection method a bit, by retesting for hotspots after removing counties with no twitter activity at all. countiesNo0 &lt;- readRDS(here(&quot;data_public&quot;, &quot;counties_tweet_counts.RDS&quot;)) |&gt; filter(tweet_rate &gt; 0) Recreate the spatial weights matrix county_coords &lt;- countiesNo0 %&gt;% st_centroid() %&gt;% # convert polygons to centroid points st_coordinates() # convert to simple x,y coordinates to play with stdep thresdist &lt;- county_coords %&gt;% dnearneigh(0, 110, longlat = TRUE) %&gt;% # use geodesic distance of 110km # distance should be long enough for every feature to have &gt;= one neighbor include.self() # include a county in its own neighborhood (for G*) thresdist # view statistical summary of the nearest neighbors ## Neighbour list object: ## Number of regions: 601 ## Number of nonzero links: 7757 ## Percentage nonzero weights: 2.147558 ## Average number of links: 12.90682 ## 11 disjoint connected subgraphs Re-plot the spatial weights matrix plot(thresdist, county_coords, lwd=0.01) # plot nearest neighbor ties Re-calculate the local Getis-Ord G* Statistic dwm &lt;- nb2listw(thresdist, zero.policy = T) countiesNo0$locG &lt;- as.vector(localG(countiesNo0$tweet_rate, listw = dwm, zero.policy = TRUE )) summary(counties$locG) ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## -1.9221 -1.1916 -0.8742 -0.2236 -0.1173 13.9514 siglevel &lt;- c(1.15, 1.95) countiesNo0 &lt;- countiesNo0 |&gt; mutate(sig = cut(locG, c( min(counties$locG), siglevel[2] * -1, siglevel[1] * -1, siglevel[1], siglevel[2], max(countiesNo0$locG) ))) rm(siglevel) And, re-map the hotspots. # map results! ggplot() + geom_sf(data = countiesNo0, aes(fill = sig), color = &quot;white&quot;, lwd = 0.1) + scale_fill_manual( values = c(&quot;#0000FF80&quot;, &quot;#8080FF80&quot;, &quot;#FFFFFF80&quot;, &quot;#FF808080&quot;, &quot;#FF000080&quot;), labels = c(&quot;low&quot;, &quot;&quot;, &quot;insignificant&quot;, &quot;&quot;, &quot;high&quot;), aesthetics = &quot;fill&quot; ) + guides(fill = guide_legend(title = &quot;Hot Spots&quot;)) + labs(title = &quot;Clusters of Hurricane Dorian Twitter Activity&quot;) + theme( plot.title = element_text(hjust = 0.5), axis.title.x = element_blank(), axis.title.y = element_blank() ) As we can see, removing the large number of counties with no tweet activity changes the global average of the tweet rate. Therefore, neighborhoods must exhibit a higher threshold of twitter activity to be considered significant, reducing the overall geographic extent of the hotspots. The cold spots also shift. Whereas they were concentrated in areas with no twitter activity at all, they are now centered on areas with low but non-zero twitter activity. "],["hurricane-ida.html", "Chapter 14 Hurricane Ida", " Chapter 14 Hurricane Ida Install packages install.packages(&quot;tidyverse&quot;) install.packages(&quot;here&quot;) Load packages Download and load Hurricane Ida data, combining all three queries into a single data frame. ida_raw &lt;- data.frame() url &lt;- &quot;https://geography.middlebury.edu/jholler/wt25/ida/&quot; for(i in 1:3){ file &lt;- paste0(&quot;tevent_raw&quot;, i, &quot;.RDS&quot;) path &lt;- here(&quot;data_private&quot;, file) if(!file.exists(path)){ download.file(paste0(url, file), path, mode=&quot;wb&quot;) } ida_raw &lt;- bind_rows(ida_raw, readRDS(path)) } Is every status_id unique? Is there any redundancy in the data? ida_raw |&gt; nrow() ## [1] 504850 ida_raw |&gt; select(status_id) |&gt; distinct() |&gt; nrow() ## [1] 281981 It appears that there is redundancy. Let’s try to find the tweets with redundant status_id’s to confirm that we should discard duplicates. duplicateIDs &lt;- ida_raw |&gt; count(status_id, sort = TRUE) |&gt; head(10) ida_raw |&gt; inner_join(duplicateIDs, by = &quot;status_id&quot;) |&gt; arrange(-n, status_id) |&gt; select(status_id, text) |&gt; head(10) ## status_id ## 1 1433150471464706048 ## 2 1433150471464706048 ## 3 1433150471464706048 ## 4 1433150471464706048 ## 5 1433150471464706048 ## 6 1433150520181547009 ## 7 1433150520181547009 ## 8 1433150520181547009 ## 9 1433150520181547009 ## 10 1433172773766156290 ## text ## 1 #StCharles Parish #LA Distribution site #Ida \\nWest Bank Bridge Park. \\nSeptember 1 12pm-5pm , 8am-5pm from September 2 on\\nIce, Water and MREs provided. \\nAccess from I-310 to River Rd . Do Not Use the Levee\\nhttps://t.co/Jwws9mD9xi https://t.co/5gIK46Lfjf ## 2 #StCharles Parish #LA Distribution site #Ida \\nWest Bank Bridge Park. \\nSeptember 1 12pm-5pm , 8am-5pm from September 2 on\\nIce, Water and MREs provided. \\nAccess from I-310 to River Rd . Do Not Use the Levee\\nhttps://t.co/Jwws9mD9xi https://t.co/5gIK46Lfjf ## 3 #StCharles Parish #LA Distribution site #Ida \\nWest Bank Bridge Park. \\nSeptember 1 12pm-5pm , 8am-5pm from September 2 on\\nIce, Water and MREs provided. \\nAccess from I-310 to River Rd . Do Not Use the Levee\\nhttps://t.co/Jwws9mD9xi https://t.co/5gIK46Lfjf ## 4 #StCharles Parish #LA Distribution site #Ida \\nWest Bank Bridge Park. \\nSeptember 1 12pm-5pm , 8am-5pm from September 2 on\\nIce, Water and MREs provided. \\nAccess from I-310 to River Rd . Do Not Use the Levee\\nhttps://t.co/Jwws9mD9xi https://t.co/5gIK46Lfjf ## 5 #StCharles Parish #LA Distribution site #Ida \\nWest Bank Bridge Park. \\nSeptember 1 12pm-5pm , 8am-5pm from September 2 on\\nIce, Water and MREs provided. \\nAccess from I-310 to River Rd . Do Not Use the Levee\\nhttps://t.co/Jwws9mD9xi https://t.co/5gIK46Lfjf ## 6 @cjfaison Same here in Pennsylvania. Ida is kicking our butts with all this heavy rain and tornado warnings. ## 7 @cjfaison Same here in Pennsylvania. Ida is kicking our butts with all this heavy rain and tornado warnings. ## 8 @cjfaison Same here in Pennsylvania. Ida is kicking our butts with all this heavy rain and tornado warnings. ## 9 @cjfaison Same here in Pennsylvania. Ida is kicking our butts with all this heavy rain and tornado warnings. ## 10 I want the whole video and the follow up. Was the cow ok? Watch Workers Rescue A Cow Trapped In A Tree After Hurricane Ida https://t.co/zISrIta5pr It looks like we can keep the first instance of every unique status ID. Adding status_id to distinct keeps all the unique status_id values, but would only give the one column: status_id. .keep_all = TRUE keeps all the columns and the data values associated with the first instance of each status_id. ida_unique &lt;- ida_raw |&gt; distinct(status_id, .keep_all=TRUE) Check the number of rows and number of distinct rows in each data frame ida_unique |&gt; nrow() ## [1] 281981 ida_unique |&gt; select(status_id) |&gt; distinct() |&gt; nrow() ## [1] 281981 Now all the tweets are unique! Save this combined and unique dataset. ida_merged_file &lt;- here(&quot;data_private&quot;, &quot;ida_merged.RDS&quot;) ida_unique |&gt; saveRDS(ida_merged_file) Load Ida merged data ida_merged &lt;- readRDS(ida_merged_file) Which profiles should we geocode? Unnest geo_coords first, to avoid geocoding those ida_merged &lt;- ida_merged |&gt; unnest_wider(geo_coords, names_sep = &quot;_&quot;) Then, filter for tweets with no geo_coords and insufficiently precise place types. locations &lt;- ida_merged |&gt; filter(is.na(geo_coords_1)) |&gt; filter(!(place_type %in% c(&quot;poi&quot;, &quot;neighborhood&quot;, &quot;city&quot;))) |&gt; select(location) |&gt; count(location, sort = TRUE) head(locations, 20) ## # A tibble: 20 × 2 ## location n ## &lt;chr&gt; &lt;int&gt; ## 1 United States 13033 ## 2 New Orleans, LA 11230 ## 3 New York, NY 6443 ## 4 Washington, DC 5969 ## 5 USA 5051 ## 6 Baton Rouge, LA 3784 ## 7 Houston, TX 3764 ## 8 Louisiana, USA 3688 ## 9 Atlanta, GA 3188 ## 10 Florida, USA 2591 ## 11 New York 2558 ## 12 New Orleans 2540 ## 13 New York City 2445 ## 14 New York, USA 2427 ## 15 Philadelphia, PA 2354 ## 16 Chicago, IL 2304 ## 17 Miami, FL 2304 ## 18 California, USA 2131 ## 19 Dallas, TX 1956 ## 20 New Jersey, USA 1883 There is a lot of junk in the locations data, but it appears that there is also a lot of good data. There is also a pattern in the good data: a city followed by a comma and a state or country. Let’s filter the locations for those with commas, and not commas followed by “USA” locationsclean &lt;- locations |&gt; mutate(locclean = tolower(location), locclean = str_remove(locclean, &quot;[ ,]+u.?s.?a?&quot;), locclean = str_remove(locclean, &quot;[ ,]+united states&quot;), locclean = str_remove(locclean, &quot;[ ,]+canada&quot;), locclean = str_remove(locclean, &quot;(usa)&quot;), locclean = tolower(locclean), locclean = str_replace(locclean, &quot;d.c.&quot;, &quot;dc&quot;), locclean = str_replace(locclean, &quot;, new york&quot;, &quot;ny&quot;), locclean = str_replace(locclean, &quot;, nyc&quot;, &quot;ny&quot;)) |&gt; filter(str_detect(locclean, &quot;,&quot;)) |&gt; mutate(split = str_split(locclean, &quot;,&quot;)) |&gt; unnest_wider(split, names_sep=&quot;_&quot;) |&gt; mutate(split_3 = str_trim(split_3), split_1 = str_trim(split_1), split_2 = str_trim(split_2), city = ifelse(is.na(split_3) | !(str_length(split_3) == 2), split_1, split_2), state = ifelse(is.na(split_3) | !(str_length(split_3) == 2), split_2, split_3)) |&gt; filter(is.na(split_4), is.na(split_3) | str_length(split_3) == 2) |&gt; filter(!state %in% c(&quot;canada&quot;, &quot;mexico&quot;, &quot;méxico&quot;, &quot;estados unidos&quot; )) |&gt; mutate(state = str_remove_all(state, &quot;[:digit:]&quot;), state = str_trim(str_remove_all(state, &quot;[:punct:]&quot;)), city = str_trim(str_remove_all(city, &quot;[:digit:]&quot;))) |&gt; filter(str_length(city) &gt;=3, state %in% c(tolower(state.name), tolower(state.abb), &quot;dc&quot;, &quot;puerto rico&quot;, &quot;pr&quot;)) locations_to_code &lt;- locationsclean |&gt; count(city, state, sort = TRUE) Try to geocode and save the results (so that this process never needs to run again). locations_coded &lt;- geocode(locations_to_code, city = city, state = state) locations_coded |&gt; saveRDS(here(&quot;data_public&quot;, &quot;geocodedLocations.RDS&quot;)) Load geocoding results. locations_coded &lt;- readRDS(here(&quot;data_public&quot;, &quot;geocodedLocations.RDS&quot;)) Join geocoded locations back to location text and save as profile_geocodes.RDS locations_xy &lt;- locationsclean |&gt; left_join(locations_coded, by = c(&quot;city&quot;, &quot;state&quot;)) |&gt; select(location, prof_lat = lat, prof_long = long) locations_xy |&gt; saveRDS(here(&quot;data_public&quot;, &quot;profile_geocodes.RDS&quot;)) Join geocoded locations back to tweets ida_merged_geo &lt;- ida_unique |&gt; left_join(locations_xy, by = &quot;location&quot;) ida_merged_geo |&gt; filter(!is.na(prof_lat)) |&gt; nrow() ## [1] 157579 Now, the Hurricane Ida data can be mapped with geographic coordinates, place name, or users’ location name, so long as the place is more specific than a county and the name has a city and state separated by a comma. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
